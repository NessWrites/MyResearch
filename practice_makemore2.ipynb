{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "264142ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt #for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e830d4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "627ff30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60f2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "#build the vocabulary of characters and mappings to/from integers\n",
    "#note stoi and itos are dictionaries\n",
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#string to integers\n",
    "stoi['.'] =0 #adding \".\" to our characters which is a to z \n",
    "itos = {i:s for s,i in stoi.items()} #just keeping integers at front and characters at second\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3dff954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 1\n",
      "b 2\n",
      "c 3\n",
      "d 4\n",
      "e 5\n",
      "f 6\n",
      "g 7\n",
      "h 8\n",
      "i 9\n",
      "j 10\n",
      "k 11\n",
      "l 12\n",
      "m 13\n",
      "n 14\n",
      "o 15\n",
      "p 16\n",
      "q 17\n",
      "r 18\n",
      "s 19\n",
      "t 20\n",
      "u 21\n",
      "v 22\n",
      "w 23\n",
      "x 24\n",
      "y 25\n",
      "z 26\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "\n",
    "for i,s in enumerate(chars):\n",
    "    print(s,i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a661a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 1), ('b', 2), ('c', 3), ('d', 4), ('e', 5), ('f', 6), ('g', 7), ('h', 8), ('i', 9), ('j', 10), ('k', 11), ('l', 12), ('m', 13), ('n', 14), ('o', 15), ('p', 16), ('q', 17), ('r', 18), ('s', 19), ('t', 20), ('u', 21), ('v', 22), ('w', 23), ('x', 24), ('y', 25), ('z', 26)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#string to integers\n",
    "stoi.items() #gives list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ffc4d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 a\n",
      "2 b\n",
      "3 c\n",
      "4 d\n",
      "5 e\n",
      "6 f\n",
      "7 g\n",
      "8 h\n",
      "9 i\n",
      "10 j\n",
      "11 k\n",
      "12 l\n",
      "13 m\n",
      "14 n\n",
      "15 o\n",
      "16 p\n",
      "17 q\n",
      "18 r\n",
      "19 s\n",
      "20 t\n",
      "21 u\n",
      "22 v\n",
      "23 w\n",
      "24 x\n",
      "25 y\n",
      "26 z\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#string to integers\n",
    "for s,i in stoi.items():\n",
    "    print(i,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4f7ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ----> e\n",
      "..e ----> m\n",
      ".em ----> m\n",
      "emm ----> a\n",
      "mma ----> .\n",
      "olivia\n",
      "... ----> o\n",
      "..o ----> l\n",
      ".ol ----> i\n",
      "oli ----> v\n",
      "liv ----> i\n",
      "ivi ----> a\n",
      "via ----> .\n",
      "ava\n",
      "... ----> a\n",
      "..a ----> v\n",
      ".av ----> a\n",
      "ava ----> .\n",
      "isabella\n",
      "... ----> i\n",
      "..i ----> s\n",
      ".is ----> a\n",
      "isa ----> b\n",
      "sab ----> e\n",
      "abe ----> l\n",
      "bel ----> l\n",
      "ell ----> a\n",
      "lla ----> .\n",
      "sophia\n",
      "... ----> s\n",
      "..s ----> o\n",
      ".so ----> p\n",
      "sop ----> h\n",
      "oph ----> i\n",
      "phi ----> a\n",
      "hia ----> .\n"
     ]
    }
   ],
   "source": [
    "#build the dataset\n",
    "block_size = 3 #context length : how many characters we take to predict the next one\n",
    "X,Y = [] , [] #X is inputs and Y is labels to the inputs\n",
    "for w in words[:5]: \n",
    "    print(w)\n",
    "    context = [0]*block_size\n",
    "    context\n",
    "    for ch in w + '.': #including '.' with  all the characters in 5 names \n",
    "        ix = stoi[ch] #gets the character in sequence meaning-\n",
    "        #ix gives the number value of a,b,c,d,etc for eg for a is 1, b is 2\n",
    "        X.append(context) #stores the current running sequence\n",
    "        Y.append(ix) #stores the sequence of alphabets\n",
    "        print(''.join(itos[i] for i in context), '---->',itos[ix])\n",
    "        context = context[1:] + [ix] #crop and append: crop the context and enter the sequence\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b76c507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X #same as above but in tensor form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "969edafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b726497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y #same as above but in tensor form\n",
    "#he says Y are the labels becoz  Y is 1 for a, 2 for b , and so on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f005780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb56545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets write neural network that takes X and predicts Y\n",
    "#We have 27 possible characters and we are going to embedd them into lower dimension space (eg 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dcc313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here each one of  27 alphabetical characters will have 2 dimensional embeddings(columns)\n",
    "C = torch.randn(27,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "844aca0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0711, -0.8820],\n",
       "        [-0.7556, -0.2081],\n",
       "        [-2.0182, -1.0529],\n",
       "        [ 0.3585,  2.2162],\n",
       "        [ 0.6602, -0.3689],\n",
       "        [-0.2494, -0.5952],\n",
       "        [ 1.9089,  0.1458],\n",
       "        [-1.1049,  0.6283],\n",
       "        [-0.6471, -0.2741],\n",
       "        [-0.7328, -1.4011],\n",
       "        [-1.3276,  0.5178],\n",
       "        [-0.9175,  0.4276],\n",
       "        [ 0.2672, -0.7977],\n",
       "        [-0.9011,  0.9246],\n",
       "        [-0.0124, -0.8901],\n",
       "        [ 0.5618, -0.0989],\n",
       "        [-0.4265, -0.1173],\n",
       "        [-0.6315,  2.0224],\n",
       "        [-1.0791, -0.4604],\n",
       "        [ 0.2689,  2.0248],\n",
       "        [-0.0243,  1.1189],\n",
       "        [ 0.2205,  1.0552],\n",
       "        [ 1.1129,  0.0395],\n",
       "        [ 1.2117,  0.6793],\n",
       "        [ 0.7426,  0.0851],\n",
       "        [ 0.3389,  2.6479],\n",
       "        [ 0.0912,  0.8089]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d94c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7396, -0.2515])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#before we embed all of the integers inside the input X using the lookup take C, we will try to embed -\n",
    "#a single individual integer 5\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc58897d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes = 27)\n",
    "#revision: one hot vector gives the position of the selected element in the selected num_classes.\n",
    "#index of 5 in  a zero vectors of size 27. \n",
    "#here 27 is our size of a to z + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30818670",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "one_hot(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#now if we take the one hot vector and mulitply with c \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: one_hot(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "#now if we take the one hot vector and mulitply with c \n",
    "F.one_hot(5, num_classes = 27) #throws error \n",
    "#bcoz the parameters of one_hot vector should be in tensor not integer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3029424a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected m1 and m2 to have the same dtype, but got: long long != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#now if we take the one hot vector and mulitply with c \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected m1 and m2 to have the same dtype, but got: long long != float"
     ]
    }
   ],
   "source": [
    "#now if we take the one hot vector and mulitply with c \n",
    "F.one_hot(torch.tensor(5), num_classes = 27) @ C\n",
    "#also error because C is a float and one_hot vector is int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4693d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec0c1f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7396, -0.2515])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now if we take the one hot vector and mulitply with c \n",
    "F.one_hot(torch.tensor(5), num_classes = 27).float() @ C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "50e7eeac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7396, -0.2515])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now look at the position of 5 in result of line 39 from randn function of C.\n",
    "#above resultant  and this resultant for 5 are same\n",
    "#they are same bcoz of how matrix multiplication works\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c08136bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-0.7396, -0.2515]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7396, -0.2515],\n",
       "         [ 0.0371, -0.5359]],\n",
       "\n",
       "        [[-0.7396, -0.2515],\n",
       "         [ 0.0371, -0.5359],\n",
       "         [ 0.0371, -0.5359]],\n",
       "\n",
       "        [[ 0.0371, -0.5359],\n",
       "         [ 0.0371, -0.5359],\n",
       "         [ 0.0781,  1.0029]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-0.5786, -1.1284]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.5786, -1.1284],\n",
       "         [-0.4093,  0.8374]],\n",
       "\n",
       "        [[-0.5786, -1.1284],\n",
       "         [-0.4093,  0.8374],\n",
       "         [-1.4390,  0.1743]],\n",
       "\n",
       "        [[-0.4093,  0.8374],\n",
       "         [-1.4390,  0.1743],\n",
       "         [-0.8207,  0.5067]],\n",
       "\n",
       "        [[-1.4390,  0.1743],\n",
       "         [-0.8207,  0.5067],\n",
       "         [-1.4390,  0.1743]],\n",
       "\n",
       "        [[-0.8207,  0.5067],\n",
       "         [-1.4390,  0.1743],\n",
       "         [ 0.0781,  1.0029]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [ 0.0781,  1.0029]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [ 0.0781,  1.0029],\n",
       "         [-0.8207,  0.5067]],\n",
       "\n",
       "        [[ 0.0781,  1.0029],\n",
       "         [-0.8207,  0.5067],\n",
       "         [ 0.0781,  1.0029]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-1.4390,  0.1743]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-1.4390,  0.1743],\n",
       "         [ 0.1035,  0.9969]],\n",
       "\n",
       "        [[-1.4390,  0.1743],\n",
       "         [ 0.1035,  0.9969],\n",
       "         [ 0.0781,  1.0029]],\n",
       "\n",
       "        [[ 0.1035,  0.9969],\n",
       "         [ 0.0781,  1.0029],\n",
       "         [-0.4572, -1.4080]],\n",
       "\n",
       "        [[ 0.0781,  1.0029],\n",
       "         [-0.4572, -1.4080],\n",
       "         [-0.7396, -0.2515]],\n",
       "\n",
       "        [[-0.4572, -1.4080],\n",
       "         [-0.7396, -0.2515],\n",
       "         [-0.4093,  0.8374]],\n",
       "\n",
       "        [[-0.7396, -0.2515],\n",
       "         [-0.4093,  0.8374],\n",
       "         [-0.4093,  0.8374]],\n",
       "\n",
       "        [[-0.4093,  0.8374],\n",
       "         [-0.4093,  0.8374],\n",
       "         [ 0.0781,  1.0029]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [-0.7423, -0.9527],\n",
       "         [ 0.1035,  0.9969]],\n",
       "\n",
       "        [[-0.7423, -0.9527],\n",
       "         [ 0.1035,  0.9969],\n",
       "         [-0.5786, -1.1284]],\n",
       "\n",
       "        [[ 0.1035,  0.9969],\n",
       "         [-0.5786, -1.1284],\n",
       "         [-2.1169, -0.1252]],\n",
       "\n",
       "        [[-0.5786, -1.1284],\n",
       "         [-2.1169, -0.1252],\n",
       "         [-0.3450, -0.3041]],\n",
       "\n",
       "        [[-2.1169, -0.1252],\n",
       "         [-0.3450, -0.3041],\n",
       "         [-1.4390,  0.1743]],\n",
       "\n",
       "        [[-0.3450, -0.3041],\n",
       "         [-1.4390,  0.1743],\n",
       "         [ 0.0781,  1.0029]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but we are going to discard one_hot and use index to create and use embedding tables bcoz its much fast4er\n",
    "#embedding a single integer is fine but the issue arrises \n",
    "#how to we simultaneously embed 32*3integers (shape of X tensor) in array X\n",
    "#luckily pytorch indexing is fairly flexible and powerful, we can also index using lists and tensor also\n",
    "#we can also repeat same index\n",
    "C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f16df6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2bbc288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2173,  0.0507],\n",
       "        [-0.2756, -0.0916],\n",
       "        [-0.0777, -0.2588],\n",
       "        [ 0.7871,  0.4506],\n",
       "        [ 1.0318, -0.7871],\n",
       "        [-0.2263, -0.4044],\n",
       "        [-0.5612,  0.1794],\n",
       "        [-0.0040, -0.4620],\n",
       "        [ 1.3280,  0.5744],\n",
       "        [-0.4519,  1.2258],\n",
       "        [-0.8532,  1.0679],\n",
       "        [-1.4275, -1.2740],\n",
       "        [ 0.8664,  0.8310],\n",
       "        [-0.0469,  0.0607],\n",
       "        [-0.4976,  0.2890],\n",
       "        [ 1.4423, -0.0607],\n",
       "        [-1.8962,  1.1836],\n",
       "        [-2.0133, -1.2333],\n",
       "        [-0.8790,  0.7800],\n",
       "        [ 0.5196, -0.1504],\n",
       "        [-1.8798, -0.3277],\n",
       "        [-0.5186,  0.8508],\n",
       "        [ 0.4364, -1.2417],\n",
       "        [ 0.5866, -0.0350],\n",
       "        [ 0.0646,  1.3123],\n",
       "        [-1.3500,  0.5595],\n",
       "        [ 2.5893, -0.2545]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C =torch.randn(27,2)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0482ade9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bfca495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for example\n",
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f7c7256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7556, -0.2081])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd4c1967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7556, -0.2081])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74966afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0ddba006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9f4c7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing the hidden layer\n",
    "W1 = torch.randn((6,100)) #6 bcoz we have 2 dimensional embeddings and we have 3 neural weights\n",
    "b1 = torch.randn(100) #biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "517b9602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 100])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd0e91f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#We want to multiply the embedings with the weights and add the bias but  the embeddings are stacked on the \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#dimension of this input tensor and it wont work\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m \u001b[38;5;241m+\u001b[39mb1\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (96x2 and 6x100)"
     ]
    }
   ],
   "source": [
    "#We want to multiply the embedings with the weights and add the bias but  the embeddings are stacked on the \n",
    "#dimension of this input tensor and it wont work\n",
    "emb @ W1 +b1\n",
    "#matrix1 and matrix2 shapes cannot be multiplied (96x2 and 6x100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38a2202a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how do we tansfor 32*3 into 32*6\n",
    "emb[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f2bb0f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.2263, -0.4044],\n",
       "        [ 1.2173,  0.0507, -0.2263, -0.4044, -0.0469,  0.0607],\n",
       "        [-0.2263, -0.4044, -0.0469,  0.0607, -0.0469,  0.0607],\n",
       "        [-0.0469,  0.0607, -0.0469,  0.0607, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.4423, -0.0607],\n",
       "        [ 1.2173,  0.0507,  1.4423, -0.0607,  0.8664,  0.8310],\n",
       "        [ 1.4423, -0.0607,  0.8664,  0.8310, -0.4519,  1.2258],\n",
       "        [ 0.8664,  0.8310, -0.4519,  1.2258,  0.4364, -1.2417],\n",
       "        [-0.4519,  1.2258,  0.4364, -1.2417, -0.4519,  1.2258],\n",
       "        [ 0.4364, -1.2417, -0.4519,  1.2258, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507, -0.2756, -0.0916,  0.4364, -1.2417],\n",
       "        [-0.2756, -0.0916,  0.4364, -1.2417, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.4519,  1.2258],\n",
       "        [ 1.2173,  0.0507, -0.4519,  1.2258,  0.5196, -0.1504],\n",
       "        [-0.4519,  1.2258,  0.5196, -0.1504, -0.2756, -0.0916],\n",
       "        [ 0.5196, -0.1504, -0.2756, -0.0916, -0.0777, -0.2588],\n",
       "        [-0.2756, -0.0916, -0.0777, -0.2588, -0.2263, -0.4044],\n",
       "        [-0.0777, -0.2588, -0.2263, -0.4044,  0.8664,  0.8310],\n",
       "        [-0.2263, -0.4044,  0.8664,  0.8310,  0.8664,  0.8310],\n",
       "        [ 0.8664,  0.8310,  0.8664,  0.8310, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  0.5196, -0.1504],\n",
       "        [ 1.2173,  0.0507,  0.5196, -0.1504,  1.4423, -0.0607],\n",
       "        [ 0.5196, -0.1504,  1.4423, -0.0607, -1.8962,  1.1836],\n",
       "        [ 1.4423, -0.0607, -1.8962,  1.1836,  1.3280,  0.5744],\n",
       "        [-1.8962,  1.1836,  1.3280,  0.5744, -0.4519,  1.2258],\n",
       "        [ 1.3280,  0.5744, -0.4519,  1.2258, -0.2756, -0.0916]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we use concatenate\n",
    "#Concatenates the given sequence of seq tensors in the given dimension. All tensors must have the same shape\n",
    "#(except in the concatenating dimension) or be empty.\n",
    "torch.cat([emb[:,0,:], emb[:,1,:], emb[:,2,:]],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f013ac09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.2263, -0.4044],\n",
       "        [ 1.2173,  0.0507, -0.2263, -0.4044, -0.0469,  0.0607],\n",
       "        [-0.2263, -0.4044, -0.0469,  0.0607, -0.0469,  0.0607],\n",
       "        [-0.0469,  0.0607, -0.0469,  0.0607, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.4423, -0.0607],\n",
       "        [ 1.2173,  0.0507,  1.4423, -0.0607,  0.8664,  0.8310],\n",
       "        [ 1.4423, -0.0607,  0.8664,  0.8310, -0.4519,  1.2258],\n",
       "        [ 0.8664,  0.8310, -0.4519,  1.2258,  0.4364, -1.2417],\n",
       "        [-0.4519,  1.2258,  0.4364, -1.2417, -0.4519,  1.2258],\n",
       "        [ 0.4364, -1.2417, -0.4519,  1.2258, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507, -0.2756, -0.0916,  0.4364, -1.2417],\n",
       "        [-0.2756, -0.0916,  0.4364, -1.2417, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.4519,  1.2258],\n",
       "        [ 1.2173,  0.0507, -0.4519,  1.2258,  0.5196, -0.1504],\n",
       "        [-0.4519,  1.2258,  0.5196, -0.1504, -0.2756, -0.0916],\n",
       "        [ 0.5196, -0.1504, -0.2756, -0.0916, -0.0777, -0.2588],\n",
       "        [-0.2756, -0.0916, -0.0777, -0.2588, -0.2263, -0.4044],\n",
       "        [-0.0777, -0.2588, -0.2263, -0.4044,  0.8664,  0.8310],\n",
       "        [-0.2263, -0.4044,  0.8664,  0.8310,  0.8664,  0.8310],\n",
       "        [ 0.8664,  0.8310,  0.8664,  0.8310, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  0.5196, -0.1504],\n",
       "        [ 1.2173,  0.0507,  0.5196, -0.1504,  1.4423, -0.0607],\n",
       "        [ 0.5196, -0.1504,  1.4423, -0.0607, -1.8962,  1.1836],\n",
       "        [ 1.4423, -0.0607, -1.8962,  1.1836,  1.3280,  0.5744],\n",
       "        [-1.8962,  1.1836,  1.3280,  0.5744, -0.4519,  1.2258],\n",
       "        [ 1.3280,  0.5744, -0.4519,  1.2258, -0.2756, -0.0916]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unbind: removes a tensor dimension & returns a tuple of all slices along a given dimension, already without it\n",
    "torch.cat(torch.unbind(emb,1),1)\n",
    "#both the result of 75 and 76 are same\n",
    "#concatenation  creates a whole new tensor which is a wastage of storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6b6a13b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6c0d3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69859059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "        [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(2,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7b82e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]],\n",
       "\n",
       "        [[12, 13],\n",
       "         [14, 15],\n",
       "         [16, 17]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3849e2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11],\n",
       "        [12, 13],\n",
       "        [14, 15],\n",
       "        [16, 17]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.view(9,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb68bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#note: view function can be used to rearrange the matrix element in any size in the way we want\n",
    "#very important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "586303eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b612f733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.2263, -0.4044],\n",
       "        [ 1.2173,  0.0507, -0.2263, -0.4044, -0.0469,  0.0607],\n",
       "        [-0.2263, -0.4044, -0.0469,  0.0607, -0.0469,  0.0607],\n",
       "        [-0.0469,  0.0607, -0.0469,  0.0607, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.4423, -0.0607],\n",
       "        [ 1.2173,  0.0507,  1.4423, -0.0607,  0.8664,  0.8310],\n",
       "        [ 1.4423, -0.0607,  0.8664,  0.8310, -0.4519,  1.2258],\n",
       "        [ 0.8664,  0.8310, -0.4519,  1.2258,  0.4364, -1.2417],\n",
       "        [-0.4519,  1.2258,  0.4364, -1.2417, -0.4519,  1.2258],\n",
       "        [ 0.4364, -1.2417, -0.4519,  1.2258, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507, -0.2756, -0.0916,  0.4364, -1.2417],\n",
       "        [-0.2756, -0.0916,  0.4364, -1.2417, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507, -0.4519,  1.2258],\n",
       "        [ 1.2173,  0.0507, -0.4519,  1.2258,  0.5196, -0.1504],\n",
       "        [-0.4519,  1.2258,  0.5196, -0.1504, -0.2756, -0.0916],\n",
       "        [ 0.5196, -0.1504, -0.2756, -0.0916, -0.0777, -0.2588],\n",
       "        [-0.2756, -0.0916, -0.0777, -0.2588, -0.2263, -0.4044],\n",
       "        [-0.0777, -0.2588, -0.2263, -0.4044,  0.8664,  0.8310],\n",
       "        [-0.2263, -0.4044,  0.8664,  0.8310,  0.8664,  0.8310],\n",
       "        [ 0.8664,  0.8310,  0.8664,  0.8310, -0.2756, -0.0916],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  1.2173,  0.0507],\n",
       "        [ 1.2173,  0.0507,  1.2173,  0.0507,  0.5196, -0.1504],\n",
       "        [ 1.2173,  0.0507,  0.5196, -0.1504,  1.4423, -0.0607],\n",
       "        [ 0.5196, -0.1504,  1.4423, -0.0607, -1.8962,  1.1836],\n",
       "        [ 1.4423, -0.0607, -1.8962,  1.1836,  1.3280,  0.5744],\n",
       "        [-1.8962,  1.1836,  1.3280,  0.5744, -0.4519,  1.2258],\n",
       "        [ 1.3280,  0.5744, -0.4519,  1.2258, -0.2756, -0.0916]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32,6)\n",
    "#we needed in 32*6 order which was given by view function which we can verify in following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "111b83a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#true means all the elements are equal in the matrix\n",
    "emb.view(32,6) == torch.cat(torch.unbind(emb,1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "685c04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the error in line 76 can be managed with view by\n",
    "#h= emb.view(32,6) @ W1 +b1\n",
    "#alternatively we can write above code like below\n",
    "#h =emb.view(shape[0],6)@ W1 +b1 or,\n",
    "h =emb.view(-1,6)@ W1 +b1  #here pytorch will infer what \"-1\" should be. bcoz number of elements must be the -\n",
    "#same and we have entered 6 (columns) already. pytorch will derive \"-1\" (rows) to be 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ae579091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.0095,  0.0900,  0.0771,  ..., -2.5085, -0.2951, -2.1940],\n",
       "        [-3.7089, -1.7027, -3.3995,  ...,  1.1338, -2.2981, -1.5713],\n",
       "        [-0.8601, -1.8549,  0.4749,  ...,  0.3254, -2.5204, -0.8408],\n",
       "        ...,\n",
       "        [ 4.3472, -0.1849,  5.7617,  ..., -4.6733, -0.8237, -1.5349],\n",
       "        [-5.9016, -0.2602,  0.5564,  ...,  1.9949,  5.5041,  0.6974],\n",
       "        [-0.4406, -2.0404, -1.4901,  ...,  0.5199, -2.4948, -1.1118]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e68fab44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape \n",
    "#remember we multiplied matrix of size 32*6(modified emb) @ 6*100 (W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cef0d9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for hidden layer activation use tanh. See graph\n",
    "h = torch.tanh(emb.view(-1,6)@ W1 +b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "01fd60fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "54964b3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we want to be sure of the broadcasting does what we would like\n",
    "b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "040a075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#32*100 broadcasting to 100\n",
    "#broadcasting will allign on the right , create a fake dimension which becomes 1,100 row vector\n",
    "#32,  100\n",
    "#1,   100\n",
    "\n",
    "#meaning it will copy vertically for every one of these of rows 32 and do an element wise addition\n",
    "#in this case the correct broadcasting will be happening becasue  the same bias vector will be added \n",
    "#to all the rows of the \"h\" matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a720d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating final layer\n",
    "W2 = torch.randn(100,27) # 100 bcoz b1 shape is 100 and 27 bcoz the output characters will be total of 27\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "75d88ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 +b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9f2d1c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "35ab00fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4f27108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = counts/counts.sum(1, keepdims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0fcc5db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "53535117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum() #sum 1 means it is normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa205718",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We would like to now index into the rows of probs and in each row we like to pluck out the\n",
    "#probabilities assigned to the correct character as given below by 'Y' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5878f56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8396e-11, 1.2810e-12, 1.8373e-12, 1.2422e-04, 6.5573e-06, 2.8568e-04,\n",
       "        4.7848e-04, 7.0228e-13, 2.1446e-05, 5.0172e-06, 2.2705e-09, 7.4921e-04,\n",
       "        1.1582e-05, 1.7964e-07, 4.5950e-10, 1.8584e-06, 2.0510e-07, 1.2420e-11,\n",
       "        5.5001e-02, 1.1046e-14, 3.3578e-10, 1.2421e-09, 1.9903e-04, 3.6046e-04,\n",
       "        6.8959e-12, 1.4681e-06, 7.4422e-06, 4.0519e-09, 1.6377e-04, 1.9379e-01,\n",
       "        2.3080e-03, 1.2495e-11])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32),Y] #this gives the current probabilities as assigned by this neural network with this \n",
    "#setting of weights to the correct character in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8bd6c264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32) # is an iterator from 1 to 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ded32c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we  have the actual letter that comes next, which comes from the arrays Y\n",
    "Y\n",
    "\n",
    "#Y are the  elements  after -------> in the result of line 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "618800a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.6753)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loss is logged and meaned version of line 121 which is the current probabilities assigned by neural network\n",
    "loss = -prob[torch.arange(32),Y].log().mean()\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "03ebfc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------made more respectable-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f2fbe837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape #our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2093153",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) #for reproducibility\n",
    "C = torch.randn((27,2), generator =g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator =g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "1f3c2120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) #number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f5bd3ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f878457e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25349006056785583\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1000):\n",
    "    #forward pass\n",
    "    emb = C[X] #(32,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6)@W1 +b1) #32, 100\n",
    "    logits = h@W2 +b2 #(32, 27)\n",
    "    #counts = logits.exp()\n",
    "    #prob = counts/ counts.sum(1,keepdims = True)\n",
    "    #loss = -prob[torch.arange(32), Y].log().mean()\n",
    "    #loss\n",
    "    loss = F.cross_entropy(logits,Y) #returns the same resut as our loss function above but, \n",
    "    # Efficient to use entropy and it is numerically well behaved\n",
    "    #print(loss.item())\n",
    "\n",
    "    #backwward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # same as setting it to 0 in pytorch\n",
    "    loss.backward() #to populate these gradients\n",
    "    #once we have the gradients we can do the parameters update\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "eb63e977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(logits,Y)\n",
    "#returns the same resut as our loss function, Efficient to use entropy and it is numerically well behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b780f75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([13.9257, 19.0060, 21.1507, 21.4255, 17.5853, 13.9257, 16.8309, 14.8607,\n",
       "        16.6102, 19.3434, 16.8704, 21.8065, 13.9257, 18.1495, 18.0791, 21.0530,\n",
       "        13.9257, 17.4576, 16.2182, 18.0743, 19.3343, 16.9431, 11.7467, 11.4520,\n",
       "        16.1146, 13.9257, 16.9535, 17.7748, 13.3471, 16.8707, 20.0948, 17.2168],\n",
       "       grad_fn=<MaxBackward0>),\n",
       "indices=tensor([19, 13, 13,  1,  0, 19, 12,  9, 22,  9,  1,  0, 19, 22,  1,  0, 19, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e2b762cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0b50e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can not lower the loss function to 0 bcoz when we see the example above ln32 , \n",
    "#... ----> e \n",
    "#... ----> o\n",
    "#... ----> s\n",
    "#neural network was supposed to predict e for emma , o for Olivia , and s for sophia from same \"...\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e45022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the dataset\n",
    "block_size = 3 #context length : how many characters we take to predict the next one\n",
    "X,Y = [] , [] #X is inputs and Y is labels to the inputs\n",
    "for w in words: \n",
    "    #print(w)\n",
    "    context = [0]*block_size\n",
    "    context\n",
    "    for ch in w + '.': #including '.' with  all the characters in 5 names \n",
    "        ix = stoi[ch] #gets the character in sequence meaning-\n",
    "        #ix gives the number value of a,b,c,d,etc for eg for a is 1, b is 2\n",
    "        X.append(context) #stores the current running sequence\n",
    "        Y.append(ix) #stores the sequence of alphabets\n",
    "        #print(''.join(itos[i] for i in context), '---->',itos[ix])\n",
    "        context = context[1:] + [ix] #crop and append: crop the context and enter the sequence\n",
    "        \n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb799b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5153374f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) #for reproducibility\n",
    "C = torch.randn((27,2), generator =g)\n",
    "W1 = torch.randn((6,100), generator = g)\n",
    "b1 = torch.randn(100, generator = g)\n",
    "W2 = torch.randn((100,27), generator =g)\n",
    "b2 = torch.randn(27, generator = g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5a14fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) #number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deb4d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e15146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.441070079803467\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    \n",
    "    #minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) #limiting our examples from 228146 to 32\n",
    "    \n",
    "    #forward pass\n",
    "    emb = C[X[ix]] #(32,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6)@W1 +b1) #32, 100\n",
    "    logits = h@W2 +b2 #(32, 27)\n",
    "    #counts = logits.exp()\n",
    "    #prob = counts/ counts.sum(1,keepdims = True)\n",
    "    #loss = -prob[torch.arange(32), Y].log().mean()\n",
    "    #loss\n",
    "    loss = F.cross_entropy(logits,Y[ix]) #returns the same resut as our loss function above but, \n",
    "    # Efficient to use entropy and it is numerically well behaved\n",
    "    #print(loss.item())\n",
    "\n",
    "\n",
    "    #backwward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # same as setting it to 0 in pytorch\n",
    "    loss.backward() #to populate these gradients\n",
    "    #once we have the gradients we can do the parameters update\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1*p.grad # 0.1 here is learning rate, a random number. How do we determine this number?\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bf49c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([102797, 112978, 207022, 174947,  95168, 148488, 149329, 115164, 138556,\n",
       "         34067, 102851, 202207,   8746,   8343,  15692, 216462,  23938,  37191,\n",
       "        126199, 193343, 164156,  25035,  79735,  41200,  38568, 168822,  20590,\n",
       "          3716, 138716,  89247, 153887,  20649])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0,X.shape[0],(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7236af81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9027, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] #(32,3,2)\n",
    "h = torch.tanh(emb.view(-1,6)@W1 +b1) #32, 100\n",
    "logits = h@W2 +b2 #(32, 27)\n",
    "loss = F.cross_entropy(logits,Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9537607e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011, 0.0011,\n",
       "        0.0011, 0.0011, 0.0011, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012,\n",
       "        0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0012, 0.0013, 0.0013, 0.0013,\n",
       "        0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0013, 0.0014,\n",
       "        0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014, 0.0014,\n",
       "        0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015, 0.0015,\n",
       "        0.0015, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016, 0.0016,\n",
       "        0.0016, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017, 0.0017,\n",
       "        0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0018, 0.0019,\n",
       "        0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0019, 0.0020, 0.0020,\n",
       "        0.0020, 0.0020, 0.0020, 0.0020, 0.0020, 0.0021, 0.0021, 0.0021, 0.0021,\n",
       "        0.0021, 0.0021, 0.0021, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022, 0.0022,\n",
       "        0.0022, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0023, 0.0024, 0.0024,\n",
       "        0.0024, 0.0024, 0.0024, 0.0024, 0.0025, 0.0025, 0.0025, 0.0025, 0.0025,\n",
       "        0.0025, 0.0026, 0.0026, 0.0026, 0.0026, 0.0026, 0.0027, 0.0027, 0.0027,\n",
       "        0.0027, 0.0027, 0.0027, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0029,\n",
       "        0.0029, 0.0029, 0.0029, 0.0029, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n",
       "        0.0031, 0.0031, 0.0031, 0.0031, 0.0032, 0.0032, 0.0032, 0.0032, 0.0032,\n",
       "        0.0033, 0.0033, 0.0033, 0.0033, 0.0034, 0.0034, 0.0034, 0.0034, 0.0034,\n",
       "        0.0035, 0.0035, 0.0035, 0.0035, 0.0036, 0.0036, 0.0036, 0.0036, 0.0037,\n",
       "        0.0037, 0.0037, 0.0037, 0.0038, 0.0038, 0.0038, 0.0039, 0.0039, 0.0039,\n",
       "        0.0039, 0.0040, 0.0040, 0.0040, 0.0040, 0.0041, 0.0041, 0.0041, 0.0042,\n",
       "        0.0042, 0.0042, 0.0042, 0.0043, 0.0043, 0.0043, 0.0044, 0.0044, 0.0044,\n",
       "        0.0045, 0.0045, 0.0045, 0.0045, 0.0046, 0.0046, 0.0046, 0.0047, 0.0047,\n",
       "        0.0047, 0.0048, 0.0048, 0.0048, 0.0049, 0.0049, 0.0049, 0.0050, 0.0050,\n",
       "        0.0050, 0.0051, 0.0051, 0.0051, 0.0052, 0.0052, 0.0053, 0.0053, 0.0053,\n",
       "        0.0054, 0.0054, 0.0054, 0.0055, 0.0055, 0.0056, 0.0056, 0.0056, 0.0057,\n",
       "        0.0057, 0.0058, 0.0058, 0.0058, 0.0059, 0.0059, 0.0060, 0.0060, 0.0060,\n",
       "        0.0061, 0.0061, 0.0062, 0.0062, 0.0062, 0.0063, 0.0063, 0.0064, 0.0064,\n",
       "        0.0065, 0.0065, 0.0066, 0.0066, 0.0067, 0.0067, 0.0067, 0.0068, 0.0068,\n",
       "        0.0069, 0.0069, 0.0070, 0.0070, 0.0071, 0.0071, 0.0072, 0.0072, 0.0073,\n",
       "        0.0073, 0.0074, 0.0074, 0.0075, 0.0075, 0.0076, 0.0076, 0.0077, 0.0077,\n",
       "        0.0078, 0.0079, 0.0079, 0.0080, 0.0080, 0.0081, 0.0081, 0.0082, 0.0082,\n",
       "        0.0083, 0.0084, 0.0084, 0.0085, 0.0085, 0.0086, 0.0086, 0.0087, 0.0088,\n",
       "        0.0088, 0.0089, 0.0090, 0.0090, 0.0091, 0.0091, 0.0092, 0.0093, 0.0093,\n",
       "        0.0094, 0.0095, 0.0095, 0.0096, 0.0097, 0.0097, 0.0098, 0.0099, 0.0099,\n",
       "        0.0100, 0.0101, 0.0101, 0.0102, 0.0103, 0.0104, 0.0104, 0.0105, 0.0106,\n",
       "        0.0106, 0.0107, 0.0108, 0.0109, 0.0109, 0.0110, 0.0111, 0.0112, 0.0112,\n",
       "        0.0113, 0.0114, 0.0115, 0.0116, 0.0116, 0.0117, 0.0118, 0.0119, 0.0120,\n",
       "        0.0121, 0.0121, 0.0122, 0.0123, 0.0124, 0.0125, 0.0126, 0.0127, 0.0127,\n",
       "        0.0128, 0.0129, 0.0130, 0.0131, 0.0132, 0.0133, 0.0134, 0.0135, 0.0136,\n",
       "        0.0137, 0.0137, 0.0138, 0.0139, 0.0140, 0.0141, 0.0142, 0.0143, 0.0144,\n",
       "        0.0145, 0.0146, 0.0147, 0.0148, 0.0149, 0.0150, 0.0151, 0.0152, 0.0154,\n",
       "        0.0155, 0.0156, 0.0157, 0.0158, 0.0159, 0.0160, 0.0161, 0.0162, 0.0163,\n",
       "        0.0165, 0.0166, 0.0167, 0.0168, 0.0169, 0.0170, 0.0171, 0.0173, 0.0174,\n",
       "        0.0175, 0.0176, 0.0178, 0.0179, 0.0180, 0.0181, 0.0182, 0.0184, 0.0185,\n",
       "        0.0186, 0.0188, 0.0189, 0.0190, 0.0192, 0.0193, 0.0194, 0.0196, 0.0197,\n",
       "        0.0198, 0.0200, 0.0201, 0.0202, 0.0204, 0.0205, 0.0207, 0.0208, 0.0210,\n",
       "        0.0211, 0.0212, 0.0214, 0.0215, 0.0217, 0.0218, 0.0220, 0.0221, 0.0223,\n",
       "        0.0225, 0.0226, 0.0228, 0.0229, 0.0231, 0.0232, 0.0234, 0.0236, 0.0237,\n",
       "        0.0239, 0.0241, 0.0242, 0.0244, 0.0246, 0.0247, 0.0249, 0.0251, 0.0253,\n",
       "        0.0254, 0.0256, 0.0258, 0.0260, 0.0261, 0.0263, 0.0265, 0.0267, 0.0269,\n",
       "        0.0271, 0.0273, 0.0274, 0.0276, 0.0278, 0.0280, 0.0282, 0.0284, 0.0286,\n",
       "        0.0288, 0.0290, 0.0292, 0.0294, 0.0296, 0.0298, 0.0300, 0.0302, 0.0304,\n",
       "        0.0307, 0.0309, 0.0311, 0.0313, 0.0315, 0.0317, 0.0320, 0.0322, 0.0324,\n",
       "        0.0326, 0.0328, 0.0331, 0.0333, 0.0335, 0.0338, 0.0340, 0.0342, 0.0345,\n",
       "        0.0347, 0.0350, 0.0352, 0.0354, 0.0357, 0.0359, 0.0362, 0.0364, 0.0367,\n",
       "        0.0369, 0.0372, 0.0375, 0.0377, 0.0380, 0.0382, 0.0385, 0.0388, 0.0390,\n",
       "        0.0393, 0.0396, 0.0399, 0.0401, 0.0404, 0.0407, 0.0410, 0.0413, 0.0416,\n",
       "        0.0418, 0.0421, 0.0424, 0.0427, 0.0430, 0.0433, 0.0436, 0.0439, 0.0442,\n",
       "        0.0445, 0.0448, 0.0451, 0.0455, 0.0458, 0.0461, 0.0464, 0.0467, 0.0471,\n",
       "        0.0474, 0.0477, 0.0480, 0.0484, 0.0487, 0.0491, 0.0494, 0.0497, 0.0501,\n",
       "        0.0504, 0.0508, 0.0511, 0.0515, 0.0518, 0.0522, 0.0526, 0.0529, 0.0533,\n",
       "        0.0537, 0.0540, 0.0544, 0.0548, 0.0552, 0.0556, 0.0559, 0.0563, 0.0567,\n",
       "        0.0571, 0.0575, 0.0579, 0.0583, 0.0587, 0.0591, 0.0595, 0.0599, 0.0604,\n",
       "        0.0608, 0.0612, 0.0616, 0.0621, 0.0625, 0.0629, 0.0634, 0.0638, 0.0642,\n",
       "        0.0647, 0.0651, 0.0656, 0.0660, 0.0665, 0.0670, 0.0674, 0.0679, 0.0684,\n",
       "        0.0688, 0.0693, 0.0698, 0.0703, 0.0708, 0.0713, 0.0718, 0.0723, 0.0728,\n",
       "        0.0733, 0.0738, 0.0743, 0.0748, 0.0753, 0.0758, 0.0764, 0.0769, 0.0774,\n",
       "        0.0780, 0.0785, 0.0790, 0.0796, 0.0802, 0.0807, 0.0813, 0.0818, 0.0824,\n",
       "        0.0830, 0.0835, 0.0841, 0.0847, 0.0853, 0.0859, 0.0865, 0.0871, 0.0877,\n",
       "        0.0883, 0.0889, 0.0895, 0.0901, 0.0908, 0.0914, 0.0920, 0.0927, 0.0933,\n",
       "        0.0940, 0.0946, 0.0953, 0.0959, 0.0966, 0.0973, 0.0979, 0.0986, 0.0993,\n",
       "        0.1000, 0.1007, 0.1014, 0.1021, 0.1028, 0.1035, 0.1042, 0.1050, 0.1057,\n",
       "        0.1064, 0.1072, 0.1079, 0.1087, 0.1094, 0.1102, 0.1109, 0.1117, 0.1125,\n",
       "        0.1133, 0.1140, 0.1148, 0.1156, 0.1164, 0.1172, 0.1181, 0.1189, 0.1197,\n",
       "        0.1205, 0.1214, 0.1222, 0.1231, 0.1239, 0.1248, 0.1256, 0.1265, 0.1274,\n",
       "        0.1283, 0.1292, 0.1301, 0.1310, 0.1319, 0.1328, 0.1337, 0.1346, 0.1356,\n",
       "        0.1365, 0.1374, 0.1384, 0.1394, 0.1403, 0.1413, 0.1423, 0.1433, 0.1443,\n",
       "        0.1453, 0.1463, 0.1473, 0.1483, 0.1493, 0.1504, 0.1514, 0.1525, 0.1535,\n",
       "        0.1546, 0.1557, 0.1567, 0.1578, 0.1589, 0.1600, 0.1611, 0.1623, 0.1634,\n",
       "        0.1645, 0.1657, 0.1668, 0.1680, 0.1691, 0.1703, 0.1715, 0.1727, 0.1739,\n",
       "        0.1751, 0.1763, 0.1775, 0.1788, 0.1800, 0.1812, 0.1825, 0.1838, 0.1850,\n",
       "        0.1863, 0.1876, 0.1889, 0.1902, 0.1916, 0.1929, 0.1942, 0.1956, 0.1969,\n",
       "        0.1983, 0.1997, 0.2010, 0.2024, 0.2038, 0.2053, 0.2067, 0.2081, 0.2096,\n",
       "        0.2110, 0.2125, 0.2140, 0.2154, 0.2169, 0.2184, 0.2200, 0.2215, 0.2230,\n",
       "        0.2246, 0.2261, 0.2277, 0.2293, 0.2309, 0.2325, 0.2341, 0.2357, 0.2373,\n",
       "        0.2390, 0.2406, 0.2423, 0.2440, 0.2457, 0.2474, 0.2491, 0.2508, 0.2526,\n",
       "        0.2543, 0.2561, 0.2579, 0.2597, 0.2615, 0.2633, 0.2651, 0.2669, 0.2688,\n",
       "        0.2707, 0.2725, 0.2744, 0.2763, 0.2783, 0.2802, 0.2821, 0.2841, 0.2861,\n",
       "        0.2880, 0.2900, 0.2921, 0.2941, 0.2961, 0.2982, 0.3002, 0.3023, 0.3044,\n",
       "        0.3065, 0.3087, 0.3108, 0.3130, 0.3151, 0.3173, 0.3195, 0.3217, 0.3240,\n",
       "        0.3262, 0.3285, 0.3308, 0.3331, 0.3354, 0.3377, 0.3400, 0.3424, 0.3448,\n",
       "        0.3472, 0.3496, 0.3520, 0.3544, 0.3569, 0.3594, 0.3619, 0.3644, 0.3669,\n",
       "        0.3695, 0.3720, 0.3746, 0.3772, 0.3798, 0.3825, 0.3851, 0.3878, 0.3905,\n",
       "        0.3932, 0.3959, 0.3987, 0.4014, 0.4042, 0.4070, 0.4098, 0.4127, 0.4155,\n",
       "        0.4184, 0.4213, 0.4243, 0.4272, 0.4302, 0.4331, 0.4362, 0.4392, 0.4422,\n",
       "        0.4453, 0.4484, 0.4515, 0.4546, 0.4578, 0.4610, 0.4642, 0.4674, 0.4706,\n",
       "        0.4739, 0.4772, 0.4805, 0.4838, 0.4872, 0.4906, 0.4940, 0.4974, 0.5008,\n",
       "        0.5043, 0.5078, 0.5113, 0.5149, 0.5185, 0.5221, 0.5257, 0.5293, 0.5330,\n",
       "        0.5367, 0.5404, 0.5442, 0.5479, 0.5517, 0.5556, 0.5594, 0.5633, 0.5672,\n",
       "        0.5712, 0.5751, 0.5791, 0.5831, 0.5872, 0.5913, 0.5954, 0.5995, 0.6036,\n",
       "        0.6078, 0.6120, 0.6163, 0.6206, 0.6249, 0.6292, 0.6336, 0.6380, 0.6424,\n",
       "        0.6469, 0.6513, 0.6559, 0.6604, 0.6650, 0.6696, 0.6743, 0.6789, 0.6837,\n",
       "        0.6884, 0.6932, 0.6980, 0.7028, 0.7077, 0.7126, 0.7176, 0.7225, 0.7275,\n",
       "        0.7326, 0.7377, 0.7428, 0.7480, 0.7531, 0.7584, 0.7636, 0.7689, 0.7743,\n",
       "        0.7796, 0.7850, 0.7905, 0.7960, 0.8015, 0.8071, 0.8127, 0.8183, 0.8240,\n",
       "        0.8297, 0.8355, 0.8412, 0.8471, 0.8530, 0.8589, 0.8648, 0.8708, 0.8769,\n",
       "        0.8830, 0.8891, 0.8953, 0.9015, 0.9077, 0.9140, 0.9204, 0.9268, 0.9332,\n",
       "        0.9397, 0.9462, 0.9528, 0.9594, 0.9660, 0.9727, 0.9795, 0.9863, 0.9931,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#learning rate\n",
    "\n",
    "#torch.linspace(0.001,1,1000) #this creates 1000 number in between from 0.001 to 1\n",
    "\n",
    "lre = torch.linspace(-3,0,1000) #learning rate exponent\n",
    "lrs =10**lre \n",
    "\n",
    "lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35a2fef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.618918418884277\n",
      "5.531765937805176\n",
      "5.217017650604248\n",
      "7.560355186462402\n",
      "5.312734603881836\n",
      "6.940987586975098\n",
      "4.749886512756348\n",
      "5.89370059967041\n",
      "6.576723098754883\n",
      "6.0223164558410645\n",
      "5.70329475402832\n",
      "5.867373466491699\n",
      "6.702986240386963\n",
      "6.036504745483398\n",
      "4.366550922393799\n",
      "5.931772708892822\n",
      "5.930878639221191\n",
      "6.2792277336120605\n",
      "6.141438007354736\n",
      "6.313513278961182\n",
      "5.694243431091309\n",
      "6.860470771789551\n",
      "5.283783435821533\n",
      "6.130720138549805\n",
      "7.238311290740967\n",
      "6.878584384918213\n",
      "5.6148481369018555\n",
      "6.901217937469482\n",
      "7.536644458770752\n",
      "7.643110275268555\n",
      "5.425060272216797\n",
      "7.068321704864502\n",
      "6.760891914367676\n",
      "4.8947906494140625\n",
      "6.5416460037231445\n",
      "5.734842777252197\n",
      "5.74247932434082\n",
      "6.8698649406433105\n",
      "7.13674259185791\n",
      "6.109066963195801\n",
      "5.740520000457764\n",
      "7.749599456787109\n",
      "6.789834022521973\n",
      "5.006019592285156\n",
      "5.171848297119141\n",
      "5.8871283531188965\n",
      "6.025906085968018\n",
      "5.404477119445801\n",
      "5.526925086975098\n",
      "5.7698845863342285\n",
      "6.855687618255615\n",
      "6.334926605224609\n",
      "4.031935214996338\n",
      "5.774295330047607\n",
      "5.063405990600586\n",
      "5.56550407409668\n",
      "6.71738338470459\n",
      "6.01273775100708\n",
      "6.588493347167969\n",
      "5.704960823059082\n",
      "4.883266925811768\n",
      "4.565101146697998\n",
      "4.563008785247803\n",
      "5.241732120513916\n",
      "5.177819728851318\n",
      "5.948988914489746\n",
      "6.118973255157471\n",
      "4.952668190002441\n",
      "5.709330081939697\n",
      "6.110156536102295\n",
      "5.561402320861816\n",
      "4.6438889503479\n",
      "4.813044548034668\n",
      "4.484119415283203\n",
      "5.939058303833008\n",
      "5.834646224975586\n",
      "5.952616214752197\n",
      "5.302916526794434\n",
      "5.456186771392822\n",
      "5.590059280395508\n",
      "6.050705432891846\n",
      "7.111998081207275\n",
      "6.937029838562012\n",
      "5.817966461181641\n",
      "6.214052200317383\n",
      "4.526348114013672\n",
      "6.5074028968811035\n",
      "4.797832489013672\n",
      "5.703693866729736\n",
      "6.8685407638549805\n",
      "5.147336483001709\n",
      "4.820563316345215\n",
      "5.212502956390381\n",
      "5.539210319519043\n",
      "5.358144760131836\n",
      "4.818342208862305\n",
      "5.410394191741943\n",
      "6.501961708068848\n",
      "6.03328800201416\n",
      "6.143793106079102\n",
      "5.722178936004639\n",
      "6.061301231384277\n",
      "4.199481964111328\n",
      "4.869032382965088\n",
      "5.444286346435547\n",
      "5.0337629318237305\n",
      "4.686219215393066\n",
      "6.457362174987793\n",
      "5.9803667068481445\n",
      "4.504812240600586\n",
      "5.50463342666626\n",
      "5.051690101623535\n",
      "5.282479763031006\n",
      "5.411301612854004\n",
      "6.116209983825684\n",
      "4.313874244689941\n",
      "4.6501851081848145\n",
      "4.846122741699219\n",
      "3.7438385486602783\n",
      "4.892383098602295\n",
      "5.137612819671631\n",
      "3.826916456222534\n",
      "4.9736528396606445\n",
      "5.091672897338867\n",
      "4.110151767730713\n",
      "5.998077392578125\n",
      "5.659897804260254\n",
      "5.71740198135376\n",
      "4.519073963165283\n",
      "4.5965256690979\n",
      "5.754316329956055\n",
      "7.056550025939941\n",
      "4.287363052368164\n",
      "5.776982307434082\n",
      "5.092625141143799\n",
      "4.497613906860352\n",
      "5.284704208374023\n",
      "5.66133975982666\n",
      "5.449432849884033\n",
      "5.01570987701416\n",
      "5.943453788757324\n",
      "6.868045806884766\n",
      "5.008718967437744\n",
      "4.611058235168457\n",
      "5.105860710144043\n",
      "4.005992412567139\n",
      "5.162501811981201\n",
      "4.234768390655518\n",
      "4.010658264160156\n",
      "5.73641300201416\n",
      "4.703525543212891\n",
      "4.570137023925781\n",
      "3.448054313659668\n",
      "4.585389137268066\n",
      "6.801858425140381\n",
      "5.169556140899658\n",
      "4.762203216552734\n",
      "5.277100086212158\n",
      "3.964937210083008\n",
      "3.891307830810547\n",
      "5.483312129974365\n",
      "6.0091047286987305\n",
      "6.5305352210998535\n",
      "4.140415668487549\n",
      "4.162639141082764\n",
      "4.5838236808776855\n",
      "4.657638072967529\n",
      "4.0571675300598145\n",
      "4.981430530548096\n",
      "4.326165199279785\n",
      "5.016915798187256\n",
      "3.980301856994629\n",
      "3.861663341522217\n",
      "5.039674282073975\n",
      "5.58674430847168\n",
      "4.7265825271606445\n",
      "4.406735897064209\n",
      "4.217857360839844\n",
      "4.374664306640625\n",
      "4.831883430480957\n",
      "4.812835693359375\n",
      "3.945924997329712\n",
      "3.4895529747009277\n",
      "6.558935642242432\n",
      "4.0457258224487305\n",
      "3.9694583415985107\n",
      "5.38857889175415\n",
      "3.383354902267456\n",
      "5.585106372833252\n",
      "5.251490116119385\n",
      "4.732479572296143\n",
      "5.4516072273254395\n",
      "5.913606643676758\n",
      "5.439912796020508\n",
      "4.263341903686523\n",
      "3.7931480407714844\n",
      "5.395810127258301\n",
      "3.9473376274108887\n",
      "5.193627834320068\n",
      "5.360608100891113\n",
      "5.141589641571045\n",
      "4.26308012008667\n",
      "4.053050518035889\n",
      "4.2638726234436035\n",
      "5.949857711791992\n",
      "4.745287895202637\n",
      "4.192311763763428\n",
      "3.8469698429107666\n",
      "5.158236026763916\n",
      "4.421731472015381\n",
      "4.990447521209717\n",
      "3.8964147567749023\n",
      "3.8719630241394043\n",
      "4.349418640136719\n",
      "4.366976261138916\n",
      "4.016904830932617\n",
      "4.808535099029541\n",
      "3.7474496364593506\n",
      "4.44304084777832\n",
      "4.663962364196777\n",
      "4.111495018005371\n",
      "5.40873384475708\n",
      "4.777814865112305\n",
      "4.556267738342285\n",
      "4.149011135101318\n",
      "6.465304374694824\n",
      "5.55497932434082\n",
      "4.459346294403076\n",
      "3.864001750946045\n",
      "3.8726768493652344\n",
      "3.9104981422424316\n",
      "3.989262104034424\n",
      "3.8918955326080322\n",
      "4.651907920837402\n",
      "5.719607830047607\n",
      "4.475156784057617\n",
      "4.588413238525391\n",
      "3.6586098670959473\n",
      "2.8879153728485107\n",
      "4.754887104034424\n",
      "4.074605941772461\n",
      "4.400984764099121\n",
      "4.3661932945251465\n",
      "4.036046504974365\n",
      "4.070733070373535\n",
      "3.7815959453582764\n",
      "4.475147724151611\n",
      "4.65479850769043\n",
      "4.004161357879639\n",
      "4.853736877441406\n",
      "4.9480695724487305\n",
      "4.494927406311035\n",
      "2.795860767364502\n",
      "4.705162048339844\n",
      "4.3027167320251465\n",
      "4.126810550689697\n",
      "3.9299206733703613\n",
      "4.084441184997559\n",
      "3.023637533187866\n",
      "4.194141387939453\n",
      "5.407209396362305\n",
      "4.250126838684082\n",
      "4.154102802276611\n",
      "3.525390148162842\n",
      "3.277771472930908\n",
      "3.7286479473114014\n",
      "4.3343729972839355\n",
      "4.344133377075195\n",
      "3.7950499057769775\n",
      "4.368149280548096\n",
      "4.009321689605713\n",
      "4.784762382507324\n",
      "3.1002657413482666\n",
      "4.164167881011963\n",
      "3.496265411376953\n",
      "4.069141864776611\n",
      "3.740900993347168\n",
      "3.328813314437866\n",
      "3.7019670009613037\n",
      "3.3612356185913086\n",
      "3.9273202419281006\n",
      "4.245063304901123\n",
      "3.214661121368408\n",
      "4.3517680168151855\n",
      "5.565272331237793\n",
      "5.186017990112305\n",
      "4.559988498687744\n",
      "3.113285779953003\n",
      "4.129322528839111\n",
      "2.9155776500701904\n",
      "5.4843058586120605\n",
      "3.2541604042053223\n",
      "3.5730783939361572\n",
      "3.9911630153656006\n",
      "4.02619743347168\n",
      "3.2328412532806396\n",
      "4.001507759094238\n",
      "4.3628973960876465\n",
      "3.6094770431518555\n",
      "3.050333023071289\n",
      "3.7278475761413574\n",
      "3.4395766258239746\n",
      "4.467437267303467\n",
      "5.009099960327148\n",
      "3.1568644046783447\n",
      "4.023775577545166\n",
      "3.707564353942871\n",
      "3.80440092086792\n",
      "4.4200968742370605\n",
      "3.666095733642578\n",
      "3.375476121902466\n",
      "3.4639973640441895\n",
      "4.394256591796875\n",
      "3.876882553100586\n",
      "3.5713539123535156\n",
      "3.78951096534729\n",
      "4.1142497062683105\n",
      "3.958928108215332\n",
      "3.3471462726593018\n",
      "4.223443031311035\n",
      "2.7923941612243652\n",
      "2.603785991668701\n",
      "4.659649848937988\n",
      "3.517514228820801\n",
      "2.993788719177246\n",
      "4.290000915527344\n",
      "2.9471938610076904\n",
      "4.652568817138672\n",
      "3.5336718559265137\n",
      "5.453052997589111\n",
      "3.327630043029785\n",
      "3.8218533992767334\n",
      "2.7268104553222656\n",
      "4.174875259399414\n",
      "2.941675901412964\n",
      "3.021252393722534\n",
      "4.700219631195068\n",
      "3.795982599258423\n",
      "3.7407987117767334\n",
      "3.4584813117980957\n",
      "4.226123809814453\n",
      "3.5356247425079346\n",
      "3.1623129844665527\n",
      "3.7236809730529785\n",
      "3.3142974376678467\n",
      "3.5952892303466797\n",
      "4.697361946105957\n",
      "3.540348529815674\n",
      "3.4480490684509277\n",
      "3.459928512573242\n",
      "3.856210470199585\n",
      "3.4426116943359375\n",
      "4.093050956726074\n",
      "3.7669410705566406\n",
      "3.0409529209136963\n",
      "4.519510746002197\n",
      "3.961480140686035\n",
      "3.5043115615844727\n",
      "2.6294915676116943\n",
      "4.232542037963867\n",
      "3.447504997253418\n",
      "3.883868932723999\n",
      "3.946075201034546\n",
      "4.848592758178711\n",
      "2.7504940032958984\n",
      "3.5367770195007324\n",
      "3.5471246242523193\n",
      "2.93552303314209\n",
      "3.495304584503174\n",
      "3.911658763885498\n",
      "2.726950168609619\n",
      "3.4114346504211426\n",
      "3.726555109024048\n",
      "3.5313827991485596\n",
      "3.478612184524536\n",
      "3.845923662185669\n",
      "3.576836109161377\n",
      "3.2444589138031006\n",
      "3.361802101135254\n",
      "3.5495526790618896\n",
      "3.1553072929382324\n",
      "2.9521288871765137\n",
      "3.4942097663879395\n",
      "4.091682434082031\n",
      "3.141465187072754\n",
      "3.1532835960388184\n",
      "3.266343355178833\n",
      "2.5435845851898193\n",
      "3.3971519470214844\n",
      "3.6219749450683594\n",
      "2.809234142303467\n",
      "4.067265510559082\n",
      "3.4916231632232666\n",
      "2.456754684448242\n",
      "3.890899181365967\n",
      "4.0440497398376465\n",
      "2.671759605407715\n",
      "3.2564752101898193\n",
      "3.7123382091522217\n",
      "3.36806583404541\n",
      "3.224824905395508\n",
      "3.800713300704956\n",
      "3.3095057010650635\n",
      "3.544668436050415\n",
      "3.572746753692627\n",
      "2.9373064041137695\n",
      "3.10827898979187\n",
      "3.2347376346588135\n",
      "3.2196712493896484\n",
      "2.449838876724243\n",
      "2.5850207805633545\n",
      "3.1204135417938232\n",
      "3.107572555541992\n",
      "2.6161293983459473\n",
      "3.2666351795196533\n",
      "2.957418203353882\n",
      "2.984241485595703\n",
      "3.1576900482177734\n",
      "2.9759793281555176\n",
      "3.728043794631958\n",
      "2.834648847579956\n",
      "3.699004650115967\n",
      "3.1354966163635254\n",
      "3.9657394886016846\n",
      "3.1081223487854004\n",
      "2.8137049674987793\n",
      "3.00416898727417\n",
      "3.1535277366638184\n",
      "3.166675567626953\n",
      "3.926665782928467\n",
      "3.068239212036133\n",
      "3.5352425575256348\n",
      "2.5631821155548096\n",
      "3.6530613899230957\n",
      "2.983511209487915\n",
      "3.8106868267059326\n",
      "3.2429111003875732\n",
      "3.054507255554199\n",
      "4.1758880615234375\n",
      "4.307497978210449\n",
      "3.103217124938965\n",
      "2.9489078521728516\n",
      "2.6291003227233887\n",
      "3.128046751022339\n",
      "2.84190034866333\n",
      "3.5077641010284424\n",
      "3.0398764610290527\n",
      "3.574906587600708\n",
      "3.2570934295654297\n",
      "2.6254892349243164\n",
      "2.776235580444336\n",
      "2.829362392425537\n",
      "3.2132890224456787\n",
      "2.3263282775878906\n",
      "3.093294858932495\n",
      "2.7737367153167725\n",
      "3.4222967624664307\n",
      "2.892550230026245\n",
      "2.873900890350342\n",
      "2.494692087173462\n",
      "2.7110650539398193\n",
      "2.9005355834960938\n",
      "2.9274420738220215\n",
      "3.273500442504883\n",
      "3.6833271980285645\n",
      "3.3355154991149902\n",
      "3.3817026615142822\n",
      "3.2494969367980957\n",
      "3.216162919998169\n",
      "2.7558836936950684\n",
      "2.631598949432373\n",
      "3.0615267753601074\n",
      "3.0238938331604004\n",
      "3.2938954830169678\n",
      "2.9071640968322754\n",
      "2.9974350929260254\n",
      "3.2939469814300537\n",
      "2.9450058937072754\n",
      "3.2171976566314697\n",
      "3.5869557857513428\n",
      "2.637463331222534\n",
      "2.755540370941162\n",
      "2.689822196960449\n",
      "3.439490556716919\n",
      "2.627380609512329\n",
      "3.2950336933135986\n",
      "3.4448459148406982\n",
      "3.330545663833618\n",
      "2.7880399227142334\n",
      "3.590439558029175\n",
      "2.849067449569702\n",
      "3.3332135677337646\n",
      "3.2552855014801025\n",
      "3.5261497497558594\n",
      "2.3550145626068115\n",
      "3.212104558944702\n",
      "3.020756721496582\n",
      "2.961939811706543\n",
      "2.55477237701416\n",
      "3.247866153717041\n",
      "2.9615824222564697\n",
      "3.509256362915039\n",
      "2.7665178775787354\n",
      "3.5177907943725586\n",
      "2.5853805541992188\n",
      "2.6759109497070312\n",
      "2.75169038772583\n",
      "2.5225400924682617\n",
      "2.674755096435547\n",
      "3.112182140350342\n",
      "2.7541842460632324\n",
      "3.16898512840271\n",
      "2.8190324306488037\n",
      "2.785780906677246\n",
      "2.965362548828125\n",
      "2.890064239501953\n",
      "2.842735528945923\n",
      "3.4840030670166016\n",
      "3.0005807876586914\n",
      "2.650836944580078\n",
      "3.1261825561523438\n",
      "2.9922280311584473\n",
      "2.7976768016815186\n",
      "3.0949316024780273\n",
      "2.930243730545044\n",
      "2.8456168174743652\n",
      "2.8049476146698\n",
      "2.7928223609924316\n",
      "2.9215617179870605\n",
      "3.2257063388824463\n",
      "2.623398542404175\n",
      "2.6023411750793457\n",
      "2.5146877765655518\n",
      "2.885695457458496\n",
      "2.3378379344940186\n",
      "2.952120065689087\n",
      "2.8120341300964355\n",
      "2.926579713821411\n",
      "2.927932024002075\n",
      "2.626094341278076\n",
      "2.5607595443725586\n",
      "2.4679126739501953\n",
      "2.818305730819702\n",
      "2.593487024307251\n",
      "3.4188976287841797\n",
      "2.9723896980285645\n",
      "2.619816303253174\n",
      "2.4036898612976074\n",
      "2.711402654647827\n",
      "3.0853521823883057\n",
      "2.617490768432617\n",
      "2.807676076889038\n",
      "2.4932196140289307\n",
      "2.958014965057373\n",
      "2.643300771713257\n",
      "2.982455253601074\n",
      "2.694366931915283\n",
      "3.385542631149292\n",
      "2.578542709350586\n",
      "2.2103729248046875\n",
      "2.699277877807617\n",
      "2.651587724685669\n",
      "2.6012299060821533\n",
      "2.6876440048217773\n",
      "2.975339889526367\n",
      "2.5615508556365967\n",
      "2.7118148803710938\n",
      "2.5847086906433105\n",
      "2.9546079635620117\n",
      "2.470132827758789\n",
      "2.6293628215789795\n",
      "2.945134162902832\n",
      "2.9554152488708496\n",
      "2.632084608078003\n",
      "2.4103736877441406\n",
      "2.4751193523406982\n",
      "2.885429620742798\n",
      "2.282275915145874\n",
      "2.6503827571868896\n",
      "2.252948760986328\n",
      "2.321850538253784\n",
      "2.4022254943847656\n",
      "2.6509509086608887\n",
      "2.435554265975952\n",
      "2.326876163482666\n",
      "2.651097297668457\n",
      "2.6670472621917725\n",
      "3.1217198371887207\n",
      "2.5718250274658203\n",
      "2.3514130115509033\n",
      "2.4579918384552\n",
      "2.285607099533081\n",
      "3.095777988433838\n",
      "3.1570534706115723\n",
      "2.6595888137817383\n",
      "2.453209638595581\n",
      "2.885439872741699\n",
      "2.4869301319122314\n",
      "2.7998292446136475\n",
      "2.801236391067505\n",
      "2.4178266525268555\n",
      "2.7837932109832764\n",
      "2.596069574356079\n",
      "2.493023633956909\n",
      "2.3494713306427\n",
      "2.3025870323181152\n",
      "2.597499370574951\n",
      "3.128558397293091\n",
      "2.522737741470337\n",
      "2.8795228004455566\n",
      "2.5421841144561768\n",
      "2.3910343647003174\n",
      "2.6945841312408447\n",
      "2.667459487915039\n",
      "2.5049028396606445\n",
      "2.722031831741333\n",
      "2.556645631790161\n",
      "2.781634569168091\n",
      "2.177194833755493\n",
      "2.4356205463409424\n",
      "2.598613739013672\n",
      "2.4679322242736816\n",
      "2.666025161743164\n",
      "2.748391628265381\n",
      "2.5828442573547363\n",
      "2.49727725982666\n",
      "2.961153268814087\n",
      "2.8214187622070312\n",
      "2.373844623565674\n",
      "2.9257731437683105\n",
      "3.2809436321258545\n",
      "2.4892680644989014\n",
      "2.4450523853302\n",
      "2.5695598125457764\n",
      "2.906491279602051\n",
      "2.4550135135650635\n",
      "2.546858549118042\n",
      "2.9098541736602783\n",
      "2.510509490966797\n",
      "2.3408894538879395\n",
      "2.3860182762145996\n",
      "2.701310396194458\n",
      "2.8253087997436523\n",
      "2.440457582473755\n",
      "2.837951183319092\n",
      "2.287261962890625\n",
      "3.1440539360046387\n",
      "2.34763240814209\n",
      "2.6038548946380615\n",
      "2.6360363960266113\n",
      "2.662804126739502\n",
      "2.52821683883667\n",
      "2.728797197341919\n",
      "2.5104002952575684\n",
      "2.7965667247772217\n",
      "2.6602118015289307\n",
      "2.516061305999756\n",
      "2.7865192890167236\n",
      "2.597644329071045\n",
      "2.356394052505493\n",
      "2.6545119285583496\n",
      "2.4283909797668457\n",
      "2.9880800247192383\n",
      "2.337127208709717\n",
      "2.6897165775299072\n",
      "2.5277068614959717\n",
      "2.464449405670166\n",
      "2.462538480758667\n",
      "2.681077480316162\n",
      "2.5915091037750244\n",
      "2.82452654838562\n",
      "2.5183804035186768\n",
      "2.941547155380249\n",
      "2.558659553527832\n",
      "2.681807518005371\n",
      "3.2129199504852295\n",
      "2.35813570022583\n",
      "2.613218069076538\n",
      "3.0021700859069824\n",
      "2.5394318103790283\n",
      "2.74397873878479\n",
      "3.4112658500671387\n",
      "2.664616107940674\n",
      "2.4463307857513428\n",
      "2.2933287620544434\n",
      "2.98527193069458\n",
      "2.576282501220703\n",
      "2.4018123149871826\n",
      "2.6540799140930176\n",
      "3.0477418899536133\n",
      "2.551396608352661\n",
      "3.1796817779541016\n",
      "2.8388426303863525\n",
      "2.6298415660858154\n",
      "2.319967269897461\n",
      "2.1973400115966797\n",
      "2.804523229598999\n",
      "2.7645461559295654\n",
      "2.6346700191497803\n",
      "2.2718327045440674\n",
      "2.208956241607666\n",
      "2.72012996673584\n",
      "2.736236572265625\n",
      "2.363128900527954\n",
      "2.9452626705169678\n",
      "2.5290255546569824\n",
      "2.6396355628967285\n",
      "2.4526007175445557\n",
      "2.3936214447021484\n",
      "3.150495767593384\n",
      "2.5411088466644287\n",
      "2.9166901111602783\n",
      "2.898784875869751\n",
      "2.376189708709717\n",
      "2.612281322479248\n",
      "2.453598976135254\n",
      "2.886951446533203\n",
      "2.5035195350646973\n",
      "3.1490137577056885\n",
      "2.4402897357940674\n",
      "2.804170608520508\n",
      "2.307082414627075\n",
      "2.8012475967407227\n",
      "2.462307929992676\n",
      "2.783433198928833\n",
      "2.4692509174346924\n",
      "2.235086679458618\n",
      "2.2062418460845947\n",
      "2.342761516571045\n",
      "2.620943784713745\n",
      "2.460503339767456\n",
      "2.2009031772613525\n",
      "2.722501277923584\n",
      "2.373666763305664\n",
      "2.1225790977478027\n",
      "2.5772364139556885\n",
      "2.8323867321014404\n",
      "2.3161942958831787\n",
      "2.6062004566192627\n",
      "2.924492597579956\n",
      "2.7688710689544678\n",
      "2.768385887145996\n",
      "2.7442150115966797\n",
      "2.6435089111328125\n",
      "2.3955368995666504\n",
      "3.1536521911621094\n",
      "2.4296040534973145\n",
      "2.878295421600342\n",
      "2.8759610652923584\n",
      "2.997626304626465\n",
      "2.9863176345825195\n",
      "2.5978658199310303\n",
      "3.1684157848358154\n",
      "3.0287368297576904\n",
      "2.4768292903900146\n",
      "2.673891305923462\n",
      "2.2827067375183105\n",
      "2.8082242012023926\n",
      "2.8641958236694336\n",
      "2.733696222305298\n",
      "2.510782480239868\n",
      "2.5657174587249756\n",
      "2.1737260818481445\n",
      "2.4347691535949707\n",
      "2.6636130809783936\n",
      "2.7537105083465576\n",
      "2.492832660675049\n",
      "2.1856253147125244\n",
      "2.825057029724121\n",
      "2.915067434310913\n",
      "2.693990468978882\n",
      "2.7032318115234375\n",
      "2.7367899417877197\n",
      "2.4734184741973877\n",
      "2.558516263961792\n",
      "2.3767993450164795\n",
      "2.680389881134033\n",
      "2.5426387786865234\n",
      "2.571863889694214\n",
      "2.437758684158325\n",
      "2.689784049987793\n",
      "2.4425458908081055\n",
      "2.6240618228912354\n",
      "2.816864013671875\n",
      "2.6370441913604736\n",
      "2.764897584915161\n",
      "2.9216222763061523\n",
      "2.8625268936157227\n",
      "3.182908535003662\n",
      "2.6292216777801514\n",
      "2.6603469848632812\n",
      "2.467867612838745\n",
      "2.60159969329834\n",
      "1.9698563814163208\n",
      "2.1979002952575684\n",
      "2.7334189414978027\n",
      "2.254826545715332\n",
      "2.915580987930298\n",
      "2.6511402130126953\n",
      "3.060997486114502\n",
      "2.943019151687622\n",
      "2.6708102226257324\n",
      "2.497171401977539\n",
      "2.8006269931793213\n",
      "2.78761625289917\n",
      "2.6218698024749756\n",
      "2.574052095413208\n",
      "2.5239317417144775\n",
      "2.4559617042541504\n",
      "2.621549606323242\n",
      "2.7341864109039307\n",
      "2.94024658203125\n",
      "2.4505035877227783\n",
      "2.787590503692627\n",
      "3.1552088260650635\n",
      "2.78190541267395\n",
      "2.670496940612793\n",
      "2.9189600944519043\n",
      "2.357541084289551\n",
      "2.9181406497955322\n",
      "2.754824161529541\n",
      "2.736023426055908\n",
      "2.7650654315948486\n",
      "2.6683452129364014\n",
      "2.4294097423553467\n",
      "2.815084457397461\n",
      "2.763214349746704\n",
      "2.3377768993377686\n",
      "2.61811900138855\n",
      "2.6164450645446777\n",
      "2.8017263412475586\n",
      "2.7117416858673096\n",
      "2.7519237995147705\n",
      "2.463290214538574\n",
      "2.372469902038574\n",
      "3.130227565765381\n",
      "2.9242262840270996\n",
      "2.493668794631958\n",
      "2.809715986251831\n",
      "2.4534103870391846\n",
      "2.5859153270721436\n",
      "2.4858651161193848\n",
      "3.0421924591064453\n",
      "3.0582761764526367\n",
      "2.5711848735809326\n",
      "2.7038557529449463\n",
      "2.6990113258361816\n",
      "2.614793300628662\n",
      "2.397265911102295\n",
      "2.6719560623168945\n",
      "2.285822629928589\n",
      "2.439072608947754\n",
      "2.282695770263672\n",
      "2.608306884765625\n",
      "2.5223989486694336\n",
      "2.7066047191619873\n",
      "3.0812864303588867\n",
      "2.6355180740356445\n",
      "2.8664345741271973\n",
      "2.467327833175659\n",
      "2.739459991455078\n",
      "2.6743242740631104\n",
      "2.4317471981048584\n",
      "2.942350149154663\n",
      "2.6493635177612305\n",
      "2.6817831993103027\n",
      "3.149658679962158\n",
      "2.479893684387207\n",
      "2.397926092147827\n",
      "2.419259548187256\n",
      "2.920161008834839\n",
      "2.7518575191497803\n",
      "2.6558895111083984\n",
      "2.880965232849121\n",
      "2.6500771045684814\n",
      "2.4672698974609375\n",
      "2.472437620162964\n",
      "2.614790678024292\n",
      "2.5385098457336426\n",
      "2.5261828899383545\n",
      "3.3155269622802734\n",
      "2.1329030990600586\n",
      "2.6646597385406494\n",
      "2.9463388919830322\n",
      "2.622159242630005\n",
      "2.428189516067505\n",
      "2.937755823135376\n",
      "2.758223295211792\n",
      "2.5320022106170654\n",
      "3.419132709503174\n",
      "3.016066074371338\n",
      "2.781656503677368\n",
      "3.1844089031219482\n",
      "2.3524796962738037\n",
      "2.302086353302002\n",
      "2.725497245788574\n",
      "2.9320356845855713\n",
      "2.636533260345459\n",
      "2.723026752471924\n",
      "3.2140421867370605\n",
      "3.249239921569824\n",
      "3.4147112369537354\n",
      "3.2042505741119385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9628005027770996\n",
      "2.946547746658325\n",
      "2.7835214138031006\n",
      "3.112327814102173\n",
      "2.4086697101593018\n",
      "3.0801146030426025\n",
      "3.5310473442077637\n",
      "2.8463001251220703\n",
      "2.8264613151550293\n",
      "2.7928414344787598\n",
      "2.6843509674072266\n",
      "3.114028215408325\n",
      "2.8448803424835205\n",
      "3.0810701847076416\n",
      "3.337813138961792\n",
      "3.094388246536255\n",
      "2.431325912475586\n",
      "3.0542805194854736\n",
      "2.7208199501037598\n",
      "2.6570916175842285\n",
      "2.5642309188842773\n",
      "3.0079267024993896\n",
      "2.867507219314575\n",
      "3.1422295570373535\n",
      "3.367126703262329\n",
      "3.5203044414520264\n",
      "3.073232412338257\n",
      "2.8421683311462402\n",
      "3.0224716663360596\n",
      "3.040945529937744\n",
      "3.1802899837493896\n",
      "2.4497060775756836\n",
      "3.2682366371154785\n",
      "2.6895735263824463\n",
      "2.7272794246673584\n",
      "2.3400282859802246\n",
      "2.9913511276245117\n",
      "3.4988903999328613\n",
      "3.944211959838867\n",
      "3.0684139728546143\n",
      "2.8737521171569824\n",
      "4.182636737823486\n",
      "2.532745361328125\n",
      "3.7090635299682617\n",
      "3.8132541179656982\n",
      "3.910233974456787\n",
      "5.321201801300049\n",
      "2.9628806114196777\n",
      "3.3566503524780273\n",
      "4.3377251625061035\n",
      "3.2095885276794434\n",
      "3.8137869834899902\n",
      "3.768899440765381\n",
      "3.6166725158691406\n",
      "3.3495230674743652\n",
      "2.860927104949951\n",
      "3.624877691268921\n",
      "3.6272776126861572\n",
      "3.622227668762207\n",
      "4.217591285705566\n",
      "2.868020534515381\n",
      "3.369235038757324\n",
      "3.3566653728485107\n",
      "4.039602279663086\n",
      "3.555036783218384\n",
      "3.744173288345337\n",
      "3.5709924697875977\n",
      "3.193331003189087\n",
      "3.024839162826538\n",
      "3.6170458793640137\n",
      "3.492917776107788\n",
      "3.1332569122314453\n",
      "3.1479785442352295\n",
      "3.8783156871795654\n",
      "3.0708224773406982\n",
      "4.052770614624023\n",
      "3.3516762256622314\n",
      "3.3247175216674805\n",
      "4.070667266845703\n",
      "4.016860485076904\n",
      "3.925678253173828\n",
      "4.99156379699707\n",
      "3.997567892074585\n",
      "4.032566070556641\n",
      "4.998817443847656\n",
      "4.684436321258545\n",
      "4.0467729568481445\n",
      "4.186652183532715\n",
      "3.7513349056243896\n",
      "3.5278029441833496\n",
      "4.050139427185059\n",
      "4.191194534301758\n",
      "3.8908333778381348\n",
      "3.566026210784912\n",
      "3.3135321140289307\n",
      "3.7262990474700928\n",
      "3.741025686264038\n"
     ]
    }
   ],
   "source": [
    "lri=[]\n",
    "lossi = []\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    #minibatch construct\n",
    "    ix = torch.randint(0, X.shape[0], (32,)) #limiting our examples from 228146 to 32\n",
    "    \n",
    "    #forward pass\n",
    "    emb = C[X[ix]] #(32,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6)@W1 +b1) #32, 100\n",
    "    logits = h@W2 +b2 #(32, 27)\n",
    "    #counts = logits.exp()\n",
    "    #prob = counts/ counts.sum(1,keepdims = True)\n",
    "    #loss = -prob[torch.arange(32), Y].log().mean()\n",
    "    #loss\n",
    "    loss = F.cross_entropy(logits,Y[ix]) #returns the same resut as our loss function above but, \n",
    "    # Efficient to use entropy and it is numerically well behaved\n",
    "    print(loss.item())\n",
    "\n",
    "\n",
    "    #backwward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # same as setting it to 0 in pytorch\n",
    "    loss.backward() #to populate these gradients\n",
    "    #once we have the gradients we can do the parameters update\n",
    "    #update\n",
    "    for p in parameters:\n",
    "        lr = lrs[i]\n",
    "        p.data += -lr*p.grad # 0.1 here is learning rate, a random number. How do we determine this number?\n",
    "    #track Stats\n",
    "    lri.append(lre[i])\n",
    "    lossi.append((loss.item()))\n",
    "        \n",
    "#print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "039ab9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x141a31fc0>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lri,lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c06069ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate decay: \n",
    "#first find the learning rate by following above \n",
    "#then start with the learning rate\n",
    "#train with it for a while\n",
    "#then apply reduce the learning rate by the factor of 10 also called the learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16daf5a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2108294218.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[43], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    is used to optimize the parameters of the model, above steps using gradient descent\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Next step would be to differentiate our data set into three stages. Because even after loss minimisation\n",
    "# we will find that the ai will repeat what data is exactly in examples and no new data will be created.\n",
    "\n",
    "#training split, dev/validation split, test split\n",
    "#80%, 10%, 10%\n",
    "\n",
    "#training split:\n",
    "is used to optimize the parameters of the model, above steps using gradient descent\n",
    "\n",
    "\n",
    "#dev/validation split:\n",
    "used for development over hyperparameters(the hidden layer/ tanh) the strength of regularization\n",
    "\n",
    "#test split:\n",
    "used to evaluate the performance of the model at the end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "not complete check the note\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
