{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makemore: part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12097\n"
     ]
    }
   ],
   "source": [
    "# MLP revisited\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 200000: 3.3239\n",
      "  10000/ 200000: 2.0322\n",
      "  20000/ 200000: 2.5675\n",
      "  30000/ 200000: 2.0125\n",
      "  40000/ 200000: 2.2446\n",
      "  50000/ 200000: 1.8897\n",
      "  60000/ 200000: 2.0785\n",
      "  70000/ 200000: 2.3681\n",
      "  80000/ 200000: 2.2918\n",
      "  90000/ 200000: 2.0238\n",
      " 100000/ 200000: 2.3673\n",
      " 110000/ 200000: 2.3132\n",
      " 120000/ 200000: 1.6414\n",
      " 130000/ 200000: 1.9311\n",
      " 140000/ 200000: 2.2231\n",
      " 150000/ 200000: 2.0027\n",
      " 160000/ 200000: 2.0997\n",
      " 170000/ 200000: 2.4949\n",
      " 180000/ 200000: 2.0199\n",
      " 190000/ 200000: 2.1707\n"
     ]
    }
   ],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1 #+ b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function.. it comprises of logits and counts.. count gives probabilities...\n",
    "  #with probabilities we can average or use any deviation method to calculate the deviation ... from which we can calculag\n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x16df19030>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the batch norm at the end of training\n",
    "\n",
    "with torch.no_grad():\n",
    "  # pass the training set through\n",
    "  emb = C[Xtr]\n",
    "  embcat = emb.view(emb.shape[0], -1)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  # measure the mean/std over the entire training set\n",
    "  bnmean = hpreact.mean(0, keepdim=True)\n",
    "  bnstd = hpreact.std(0, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  hpreact = embcat @ W1 # + b1\n",
    "  #hpreact = bngain * (hpreact - hpreact.mean(0, keepdim=True)) / hpreact.std(0, keepdim=True) + bnbias\n",
    "  hpreact = bngain * (hpreact - bnmean_running) / bnstd_running + bnbias\n",
    "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "  logits = h @ W2 + b2 # (N, vocab_size)\n",
    "  loss = F.cross_entropy(logits, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss log\n",
    "\n",
    "### original:\n",
    "train 2.1245384216308594\n",
    "val   2.168196439743042\n",
    "\n",
    "### fix softmax confidently wrong:\n",
    "train 2.07\n",
    "val   2.13\n",
    "\n",
    "### fix tanh layer too saturated at init:\n",
    "train 2.0355966091156006\n",
    "val   2.1026785373687744\n",
    "\n",
    "### use semi-principled \"kaiming init\" instead of hacky init:\n",
    "train 2.0376641750335693\n",
    "val   2.106989622116089\n",
    "\n",
    "### add batch norm layer\n",
    "train 2.0668270587921143\n",
    "val 2.104844808578491\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUMMARY + PYTORCHIFYING -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a deeper network\n",
    "# The classes we create here are the same API as nn.Module in PyTorch\n",
    "\n",
    "class Linear:\n",
    "  \n",
    "  def __init__(self, fan_in, fan_out, bias=True):\n",
    "    self.weight = torch.randn((fan_in, fan_out), generator=g) / fan_in**0.5\n",
    "    self.bias = torch.zeros(fan_out) if bias else None\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    self.out = x @ self.weight\n",
    "    if self.bias is not None:\n",
    "      self.out += self.bias\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "\n",
    "class BatchNorm1d:\n",
    "  \n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.momentum = momentum\n",
    "    self.training = True\n",
    "    # parameters (trained with backprop)\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "    # buffers (trained with a running 'momentum update')\n",
    "    self.running_mean = torch.zeros(dim)\n",
    "    self.running_var = torch.ones(dim)\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    if self.training:\n",
    "      xmean = x.mean(0, keepdim=True) # batch mean\n",
    "      xvar = x.var(0, keepdim=True) # batch variance\n",
    "    else:\n",
    "      xmean = self.running_mean\n",
    "      xvar = self.running_var\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    # update the buffers\n",
    "    if self.training:\n",
    "      with torch.no_grad():\n",
    "        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "    return self.out\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "  def __call__(self, x):\n",
    "    self.out = torch.tanh(x)\n",
    "    return self.out\n",
    "  def parameters(self):\n",
    "    return []\n",
    "\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "layers = [\n",
    "  Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "  Linear(           n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "# layers = [\n",
    "#   Linear(n_embd * block_size, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, n_hidden), Tanh(),\n",
    "#   Linear(           n_hidden, vocab_size),\n",
    "# ]\n",
    "\n",
    "with torch.no_grad():\n",
    "  # last layer: make less confident\n",
    "  layers[-1].gamma *= 0.1\n",
    "  #layers[-1].weight *= 0.1\n",
    "  # all other layers: apply gain\n",
    "  for layer in layers[:-1]:\n",
    "    if isinstance(layer, Linear):\n",
    "      layer.weight *= 1.0 #5/3\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for layer in layers:\n",
    "    layer.out.retain_grad() # AFTER_DEBUG: would take out retain_graph\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())\n",
    "  with torch.no_grad():\n",
    "    ud.append([((lr*p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "  if i >= 1000:\n",
    "    break # AFTER_DEBUG: would take out obviously to run full optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out\n",
    "    print('layer %d (%10s): mean %+.2f, std %.2f, saturated: %.2f%%' % (i, layer.__class__.__name__, t.mean(), t.std(), (t.abs() > 0.97).float().mean()*100))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('activation distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]): # note: exclude the output layer\n",
    "  if isinstance(layer, Tanh):\n",
    "    t = layer.out.grad\n",
    "    print('layer %d (%10s): mean %+f, std %e' % (i, layer.__class__.__name__, t.mean(), t.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'layer {i} ({layer.__class__.__name__}')\n",
    "plt.legend(legends);\n",
    "plt.title('gradient distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize histograms\n",
    "plt.figure(figsize=(20, 4)) # width and height of the plot\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  t = p.grad\n",
    "  if p.ndim == 2:\n",
    "    print('weight %10s | mean %+f | std %e | grad:data ratio %e' % (tuple(p.shape), t.mean(), t.std(), t.std() / p.std()))\n",
    "    hy, hx = torch.histogram(t, density=True)\n",
    "    plt.plot(hx[:-1].detach(), hy.detach())\n",
    "    legends.append(f'{i} {tuple(p.shape)}')\n",
    "plt.legend(legends)\n",
    "plt.title('weights gradient distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i,p in enumerate(parameters):\n",
    "  if p.ndim == 2:\n",
    "    plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "    legends.append('param %d' % i)\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k') # these ratios should be ~1e-3, indicate on plot\n",
    "plt.legend(legends);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "  x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xdev, Ydev),\n",
    "    'test': (Xte, Yte),\n",
    "  }[split]\n",
    "  emb = C[x] # (N, block_size, n_embd)\n",
    "  x = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "  for layer in layers:\n",
    "    x = layer(x)\n",
    "  loss = F.cross_entropy(x, y)\n",
    "  print(split, loss.item())\n",
    "\n",
    "# put layers into eval mode\n",
    "for layer in layers:\n",
    "  layer.training = False\n",
    "split_loss('train')\n",
    "split_loss('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # forward pass the neural net\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,n_embd)\n",
    "      x = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "      for layer in layers:\n",
    "        x = layer(x)\n",
    "      logits = x\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      # sample from the distribution\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      # shift the context window and track the samples\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      # if we sample the special '.' token, break\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONE; BONUS content below, not covered in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNorm forward pass as a widget\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def normshow(x0):\n",
    "  \n",
    "  g = torch.Generator().manual_seed(2147483647+1)\n",
    "  x = torch.randn(5, generator=g) * 5\n",
    "  x[0] = x0 # override the 0th example with the slider\n",
    "  mu = x.mean()\n",
    "  sig = x.std()\n",
    "  y = (x - mu)/sig\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  # plot 0\n",
    "  plt.plot([-6,6], [0,0], 'k')\n",
    "  # plot the mean and std\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, mu, sig), 'b')\n",
    "  xx = np.linspace(-6, 6, 100)\n",
    "  plt.plot(xx, stats.norm.pdf(xx, 0, 1), 'r')\n",
    "  # plot little lines connecting input and output\n",
    "  for i in range(len(x)):\n",
    "    plt.plot([x[i],y[i]], [1, 0], 'k', alpha=0.2)\n",
    "  # plot the input and output values\n",
    "  plt.scatter(x.data, torch.ones_like(x).data, c='b', s=100)\n",
    "  plt.scatter(y.data, torch.zeros_like(y).data, c='r', s=100)\n",
    "  plt.xlim(-6, 6)\n",
    "  # title\n",
    "  plt.title('input mu %.2f std %.2f' % (mu, sig))\n",
    "\n",
    "interact(normshow, x0=(-30,30,0.5));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "a = torch.randn((1000,1), requires_grad=True, generator=g)          # a.grad = b.T @ c.grad\n",
    "b = torch.randn((1000,1000), requires_grad=True, generator=g)       # b.grad = c.grad @ a.T\n",
    "c = b @ a\n",
    "loss = torch.randn(1000, generator=g) @ c\n",
    "a.retain_grad()\n",
    "b.retain_grad()\n",
    "c.retain_grad()\n",
    "loss.backward()\n",
    "print('a std:', a.std().item())\n",
    "print('b std:', b.std().item())\n",
    "print('c std:', c.std().item())\n",
    "print('-----')\n",
    "print('c grad std:', c.grad.std().item())\n",
    "print('a grad std:', a.grad.std().item())\n",
    "print('b grad std:', b.grad.std().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear + BatchNorm: activation statistics of forward and backward pass\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "n = 1000\n",
    "# linear layer ---\n",
    "inp = torch.randn(n, requires_grad=True, generator=g)\n",
    "w = torch.randn((n, n), requires_grad=True, generator=g) # / n**0.5\n",
    "x = w @ inp\n",
    "# bn layer ---\n",
    "xmean = x.mean()\n",
    "xvar = x.var()\n",
    "out = (x - xmean) / torch.sqrt(xvar + 1e-5)\n",
    "# ----\n",
    "loss = out @ torch.randn(n, generator=g)\n",
    "inp.retain_grad()\n",
    "x.retain_grad()\n",
    "w.retain_grad()\n",
    "out.retain_grad()\n",
    "loss.backward()\n",
    "\n",
    "print('inp std: ', inp.std().item())\n",
    "print('w std: ', w.std().item())\n",
    "print('x std: ', x.std().item())\n",
    "print('out std: ', out.std().item())\n",
    "print('------')\n",
    "print('out grad std: ', out.grad.std().item())\n",
    "print('x grad std: ', x.grad.std().item())\n",
    "print('w grad std: ', w.grad.std().item())\n",
    "print('inp grad std: ', inp.grad.std().item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
