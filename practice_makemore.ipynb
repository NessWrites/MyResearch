{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07fb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1621d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa0bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(len(w) for w in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5756364",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(len(w)for w in words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    print(w) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce31f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]: #gives name\n",
    "    for ch1, ch2 in zip(w, w[1:]): #gives words from name\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "name =['s', 'h', 'r', 'e','s', 't', 'h', 'a']\n",
    "cast = ['n', 'e', 's', 's']\n",
    "\n",
    "for ch1, ch2 in zip(name, cast):\n",
    "    print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d391df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:1] :\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(w, w[1:]):\n",
    "        print(ch1, ch2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26decc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example: elements can be added to the list\n",
    "name =['s', 'h', 'r', 'e','s', 't', 'h', 'a']\n",
    "another = ['t']+name + ['s']\n",
    "print(another)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        print(ch1, ch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb6e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram is a tuple of  ch1, ch2 ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918fc6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to learn the satistis of which character is going to follow the next \n",
    "#the simplest way in a bigram language mode to do so is to count. And here we use dictionary to count\n",
    "\n",
    "b = {} #dictionary\n",
    "for w in words:\n",
    "    chs = ['<S>'] +list(w)+ ['E']\n",
    "    for ch1 , ch2 in zip(chs, chs[1:]):\n",
    "        bigram =(ch1, ch2)\n",
    "        b[bigram] = b.get(bigram,0) +1 # is a method for counting the values of repeating bigrams\n",
    "        #if there are no values it returns 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(b.items(), key = lambda kv : -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87749abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros((3, 5), dtype = torch.int32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75451a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1,3]+=1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0,0] =5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5042e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27,27), dtype = torch.int32)\n",
    "N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46743f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking every possible characters from the list words \n",
    "#and passing to the set constructor becoz set removes duplicate values\n",
    "#we want lists and sort it\n",
    "chars=sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in  enumerate(chars)} # enumerate returns (0,a), (1,b), and so on... \n",
    "#stoi is a dictionary mapping characters in dictionary.\n",
    "#for eg a:0, b:1, and so on \n",
    "\n",
    "#But since we have two additional characters <S> and <E> are are gonna have to mention those seperately\n",
    "#replacing <S> and <E> with '.'\n",
    "stoi['.'] =0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796674d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#string to integers\n",
    "stoi['.'] =0 #adding \".\" to our characters which is a to z \n",
    "itos = {i:s for s,i in stoi.items()} #just keeping integers at front and characters at second\n",
    "print(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8256c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is chars \n",
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "\n",
    "for i,s in enumerate(chars):\n",
    "    print(s,i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34446b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is stoi\n",
    "#build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#string to integers\n",
    "stoi.items() #gives list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f690772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#what is itos\n",
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#string to integers\n",
    "stoi['.'] = 0\n",
    "for s,i in stoi.items():\n",
    "    print(i,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bec2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example on adding count to matrices of 0\n",
    "#First create a matrices of 0 sized 5*5\n",
    "O=  torch.zeros((5,5), dtype = torch.int32)\n",
    "O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1196950",
   "metadata": {},
   "outputs": [],
   "source": [
    "O[1,1]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5deab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce30315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now if O[1,1] is repeated it will increase the count of position  O(1,1) by that many number of times it is repeated\n",
    "O[1,1]+=1\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12025e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "O[3,3]+=1\n",
    "O[4,4]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67165e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "O[3,3].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f66af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92489872",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = ['.'] +list(w) +['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "      \n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2 ]\n",
    "     \n",
    "        N[ix1,ix2] +=1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785c773c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04194682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204b6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "N[0,1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ca1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8252c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "N[0,:].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d5ddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating probability vector\n",
    "p = N[0].float() #converting into float\n",
    "p = p/p.sum() #probablity \n",
    "p #the output now becomes the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663497cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sum() #the sum of all the probability is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3859897",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Torch.Multinomial\n",
    " #It takes tensor and returns sample of such tensor. \n",
    "    #We take sample from selected probablity distribution and we use Torch.Multinomial which\n",
    "#takes sample from multinomial probability distribution and returns integers based on the probility distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52addd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g =torch.Generator().manual_seed(2147483647) #creating a torch generator object,\n",
    "# seeding with some number that we can agree on, so that seeds  a generator, gets an object g\n",
    "\n",
    "#now we can pass above g to a function\n",
    "p = torch.rand(3, generator =g)#creates 3 random numbers using generated object to source of randomness\n",
    "p = p/p.sum()\n",
    "p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca435be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is us predicting just 1 character\n",
    "g =torch.Generator().manual_seed(214379923647)\n",
    "ix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item() #.item returns a value not tensor\n",
    "itos[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76aa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.multinomial(p, num_samples = 100, replacement = True, generator = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we predict what character follows after our first prediction\n",
    "g = torch.Generator().manual_seed(214348823647)\n",
    "out = []\n",
    "ix = 0\n",
    "while True:\n",
    "    p = N[ix].float()\n",
    "    p= p/p.sum()\n",
    "    ix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()\n",
    "    out.append(itos[ix])\n",
    "    \n",
    "    if ix == 0:\n",
    "        break\n",
    "print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b2504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we predict 10 set of different names\n",
    "g = torch.Generator().manual_seed(214379923647)\n",
    "for i in range(20):\n",
    "\t\n",
    "\tout = []\n",
    "\tix = 0\n",
    "\twhile True:\n",
    "\t\tp = N[ix].float()\n",
    "\t\tp= p/p.sum()\n",
    "\t\tix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()\n",
    "\t\tout.append(itos[ix])\n",
    "\t\t\n",
    "\t\tif ix == 0:\n",
    "\t\t\tbreak\n",
    "\tprint(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4b3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using torch.multinomial to draw samples from above\n",
    "torch.multinomial(p, num_samples = 100, replacement = True, generator = g)\n",
    "#why there are only 0,1,2 in following tensor.\n",
    "\t#because we have only generated 3 random numbers using generator above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b275d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9897d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1,keepdim = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8057e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = (N+1).float()#adding 1 here is called model smoothing\n",
    "#bcoz some bigram has probability of 0 whose log probability goes to infinity\n",
    "#to smooth our model from such error we add one.\n",
    "P /= P.sum(1, keepdim= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1bc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.ones((4,3), dtype = torch.int32).float()\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a89870",
   "metadata": {},
   "outputs": [],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf6bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumOfL=l.sum(1, keepdims = True)\n",
    "sumOfL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumOfL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb06b743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[4,3]/[4,1]\n",
    "\"\"\"\n",
    "\t\t[[1, 1, 1],\t\t\t\t[[3],\n",
    "        [1, 1, 1],\t\t/\t\t[3],\n",
    "        [1, 1, 1],\t\t\t\t[3],\n",
    "        [1, 1, 1]]\t\t\t\t[3]]\n",
    "\"\"\"\n",
    "#Result should be \n",
    "\"\"\" [[0.33],\n",
    "    [0.33],\n",
    "    [0.33]]\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dea8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l /= l.sum(1, keepdim= True)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf6c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l[1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71112ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebcf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a30311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "P.sum(1,keepdim = True).shape #column vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f73b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Is it possible to take a 27*27 array and divide  by 27*1\n",
    "#Also called broadcasting rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76887ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(214383647)\n",
    "for i in range(5):\n",
    "    out = []\n",
    "\n",
    "    ix = 0\n",
    "    while True:\n",
    "        p=P[ix] # P is a 27*27 Probability matrix\n",
    "        #p = p/p.sum()\n",
    "        #p = torch.ones(27)/27.0 #untrained model\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples = 1, replacement = True, generator = g).item()\n",
    "        \"\"\"Now the multinomial function when num_samples = 1 gives 1 probability out of the probability matrix\n",
    "        'P'\n",
    "        itos\n",
    "        \"\"\" \n",
    "        \n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words[:3]:\n",
    "    chs = ['.'] +list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1= stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1,ix2]\n",
    "        print(f'{ch1}{ch2}: {prob:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811fac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_likelihood = 0.0\n",
    "n=0\n",
    "\n",
    "for w in words:\n",
    "#for w in [\"andrejq\"]:\n",
    "    chs = ['.'] + list(w) +['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        #how can we summarize these probabilities into a single number that measure the quality of these models\n",
    "        # the term is called liklihood which is the product of all the probabilities\n",
    "        # it is telling us about the probabilities of all the data set\n",
    "        #assigned by the model that we train and this is the measure of the quality\n",
    "        #the product of these must be as high as possible when we are training these models\n",
    "        \n",
    "        #but the product of these probabilities will be very tiny number since they are less than one\n",
    "        \n",
    "        #for convinience developers use not the likelihood but the log likelihood\n",
    "        # to take the log likelihood we just have to take the log of the probability\n",
    "        #when probability tends to 1 log of proability tends to 0\n",
    "        #when probability tends to 0 log of proability tends to -4\n",
    "\n",
    "        \n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood +=logprob\n",
    "        n+=1\n",
    "        \n",
    "        #print(f'{ch1}{ch2}: {prob:.4f} {logprob:.4f}')\n",
    "        #negative log likelihood nll\n",
    "        \n",
    "        \n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll =}')\n",
    "print(f'negative log likelihood = {nll/n}')\n",
    "\n",
    "#the job of our training is to find the parameters that minimizes the negative log likelihood loss\n",
    "#Goals: Maximize likelihood of the data w.r.t model parameters (satistical modeling)\n",
    "#equivalent to maximizing the log likelihood(because log is monotonic)\n",
    "#equivalent to minimizing the negative log likelihood\n",
    "#equivalent to minimizing the average negative log likelihood\n",
    "\n",
    "#log(a*b*c) = log(a) + log(b) + log(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf359e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#casting the problem of bigram language modeling into the neural network framework\n",
    "#neural network  is still going to be the bigram model\n",
    "#it stills receives a single character as an input\n",
    "#then there is neural network with some weights or some parameters 'w'\n",
    "#it is going to output the probability distribution over the next character in sequence \n",
    "# it is going to make guesses as what is going to follow this character that was input to the model\n",
    "#in addition to that we are going to  evaluate any setting of the parameter of the neural network\n",
    "# because we have the loss function(the negative log likelihood)\n",
    "\n",
    "# So we are going to a look at its probability distribution and we are going to use the labels which\n",
    "#are basically the identity of next indentity of next character in that bigram\n",
    "\n",
    "#knowing what second character actually comes in the bigram allows us to look at how high probility the model\n",
    "#assigns to that character\n",
    "#We want that probability to be high and that is another way of saying the loss is low.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd689c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are going to use gradient -based optimization then to tune the parameters of the network\n",
    "#We have loss functions, and we are going to minimize it\n",
    "#we are going to tune the weights so that the neural net is correctly predicting the probabilities of the \n",
    "#next character\n",
    "\n",
    "#The first thing to do is to complie the training set of this neural network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the training set of all the bigrams (x,y)\n",
    "\n",
    "#This training set will be made up of two lists\n",
    "xs, ys = [],[]\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] +list(w)+['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        print(ch1,ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs) #this is first character of bigram\n",
    "ys = torch.tensor(ys) # this is second character of bigram\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031b2d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954233dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a463408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot encoding \n",
    "#A common way of encodeing integers, we create a vector where all the elements are 0 except the integer\n",
    "# and multiply it with the weights. As neural network has weights.\n",
    "\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes =27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83f8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b449d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(xenc) #if you observe the yellow dots are where ''.emma' are at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47de4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc.dtype #at default it was integer in line 77 we changed to float bcoz we want float in tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5764fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "W =torch.randn(27,1) #these are column vectors of all the alphabets\n",
    "\n",
    "#xenc is the vector of our 5 digit '.emma' filled with 0\n",
    "#xenc shape is 5*27 \n",
    "#W shape is 27*1.  We made it like it by putting parameters\n",
    "# on above randn. we took only the column part.\n",
    "# xenc@ W #is the matrix multiplication where the resultant \n",
    "# vector would be of size 5*1\n",
    "# what is happening here is we are seeing the 5 activation of neuron 'W' on this 5\n",
    "#inputs on above graph on word .emma``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ca5182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1 \n",
    "#(also called the standard normal distribution)\n",
    "W =torch.randn(27,27) #the first parameters inside are the size of the vector (a column vector)\n",
    "#and the second one is  size of neuron\n",
    "\n",
    "xenc@ W # \"@\" below is matrix multiplication\n",
    "# \n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#the result is 5*1 order matrix becoz (5,27) @ (27,1) is (5,27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc@ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc@ W).shape\n",
    "#what every element is telling here us is for every one of 27 neurons that we created\n",
    "#what is the firing rate of neurons in one of those 5 examples \".emma\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ebf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example the element 3, 13 is giving us the firing rate of 13th neuron looking at the third input\n",
    "#And the way this is achieved is by a dot product between the third input and 13th coloumn of the w matrix\n",
    "(xenc@ W)[3,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using matrix multiplication we can very efficienlty evaluate the dot product between alot oof input examples\n",
    "#in a batch\n",
    "\n",
    "#All of the neurons has weights in the columns. and matrix multiplicaton we are just doing\n",
    "#those dot products in parallels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#explanation\n",
    "xenc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "W[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc @ W)[3,13] #same result as in line 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bfd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(xenc[3]*W[:,13]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13719970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming neural network outputs into probabilities\n",
    "#By exponentiating them , all the negative numbers turns into positive and positive number turns into greater\n",
    "#positive number\n",
    "(xenc @ W).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fb4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion :\n",
    "#For every one of our five examples we now have row that came out of neural net\n",
    "#and. because of the transformation here we make sure that output of these neural networks are probabilities\n",
    "#our \"w@xenc\" give us logits and we can interpret these to be log counts\n",
    "#we expontiate counts to  look like counts and we normalize those counts to get probability distribution\n",
    "#all of these are differentiable operations\n",
    "\n",
    "#So what have we done so far is we are taking inputs, we have differentiable operations that we can -\n",
    "#back propagate through and we are getting out probatility distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5578b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = xenc@W # log-counts\n",
    "counts = logits.exp() #equivalent N\n",
    "probs = counts/counts.sum(1, keepdims = True)\n",
    "# btw: the last 2 lines counts and probs here are together called a 'softmax'\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53970591",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].sum() #The sum would be one because they are normalized\n",
    "\n",
    "#Normalization here means divided by total for probability. \n",
    "#P/P.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e98b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec76e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example:\n",
    "#We are feeding '.' in neural net\n",
    "#When we fed the '.' in neural net is when we got the index\n",
    "#then we one_hot encoded it\n",
    "#then it went into the neural net\n",
    "#out came below output which is the distribution of probabilities and its shape is just 27\n",
    "probs[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b225306",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60346ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef567d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#and we are going to interpret this this as neural net assignment for how likely every one of these 27 characters\n",
    "#are to come next\n",
    "\n",
    "#And as we tune the weights W we are going to be getting different probabilities out for any character that\n",
    "#we input, so the question is can we optimize in finding W such that probabilities coming out are pretty good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8a7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 2\n",
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782c753",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can minimize the above loss by tuning Ws by computing the gradients of the loss wrt Ws matrices\n",
    "#So we can tune W to minimize loss and find a good setting of Ws using gradient based optimization.\n",
    "\n",
    "#lets see how that works\n",
    "\n",
    "\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e53dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26440a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialize 27 neurons' weights. each neuron receives 27 inputs\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27),generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c542a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "xenc = F.one_hot(xs,num_classes = 27).float()#input to network:one hot encoding\n",
    "logits = xenc@W #predict log-counts\n",
    "counts = logits.exp()#counts equivalent to N\n",
    "probs = counts/counts.sum(1,keepdims = True) #probabilities for next character\n",
    "loss = probs[torch.arange(5),ys].log().mean()\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb707407",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cb971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backward pass\n",
    "W.grad = None #set to zero the gradient\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f818cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "W.data += 0.1*W.grad #update the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae6db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements: 228146\n"
     ]
    }
   ],
   "source": [
    "#Summary so far\n",
    "\n",
    "\n",
    "#import words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "#stoi and itos\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))#characters\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}#string to integers\n",
    "stoi['.'] =0 #adding \".\" to our characters which is a to z \n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "\n",
    "\n",
    "xs=[]\n",
    "ys =[]\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w)+ ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of elements:', num)\n",
    "\n",
    "#initiate the network\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator =g , requires_grad = True)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c58ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.371100664138794\n",
      "3.154043197631836\n",
      "3.020373821258545\n",
      "2.927711248397827\n",
      "2.8604021072387695\n",
      "2.8097290992736816\n",
      "2.7701022624969482\n",
      "2.7380728721618652\n",
      "2.711496591567993\n",
      "2.6890032291412354\n",
      "2.6696884632110596\n",
      "2.65293025970459\n",
      "2.638277292251587\n",
      "2.6253881454467773\n",
      "2.613990545272827\n",
      "2.60386323928833\n",
      "2.5948219299316406\n",
      "2.5867116451263428\n",
      "2.579403877258301\n",
      "2.572789192199707\n",
      "2.5667760372161865\n",
      "2.5612878799438477\n",
      "2.5562589168548584\n",
      "2.551633596420288\n",
      "2.547365665435791\n",
      "2.5434155464172363\n",
      "2.5397486686706543\n",
      "2.5363364219665527\n",
      "2.5331544876098633\n",
      "2.5301806926727295\n",
      "2.5273966789245605\n",
      "2.5247862339019775\n",
      "2.522334575653076\n",
      "2.520029067993164\n",
      "2.517857789993286\n",
      "2.515810489654541\n",
      "2.513878345489502\n",
      "2.512051820755005\n",
      "2.510324001312256\n",
      "2.5086867809295654\n",
      "2.5071349143981934\n",
      "2.5056610107421875\n",
      "2.5042612552642822\n",
      "2.5029289722442627\n",
      "2.5016608238220215\n",
      "2.5004520416259766\n",
      "2.4992988109588623\n",
      "2.498197317123413\n",
      "2.497144937515259\n",
      "2.496137857437134\n",
      "2.495173215866089\n",
      "2.4942495822906494\n",
      "2.49336314201355\n",
      "2.4925124645233154\n",
      "2.491694927215576\n",
      "2.4909095764160156\n",
      "2.4901537895202637\n",
      "2.4894261360168457\n",
      "2.488725423812866\n",
      "2.488049268722534\n",
      "2.4873974323272705\n",
      "2.4867680072784424\n",
      "2.4861602783203125\n",
      "2.4855730533599854\n",
      "2.4850046634674072\n",
      "2.484455108642578\n",
      "2.4839231967926025\n",
      "2.483407735824585\n",
      "2.4829084873199463\n",
      "2.482424259185791\n",
      "2.48195481300354\n",
      "2.481499195098877\n",
      "2.4810571670532227\n",
      "2.4806275367736816\n",
      "2.480210065841675\n",
      "2.479804515838623\n",
      "2.479410409927368\n",
      "2.4790267944335938\n",
      "2.4786534309387207\n",
      "2.478290557861328\n",
      "2.4779367446899414\n",
      "2.4775924682617188\n",
      "2.477257251739502\n",
      "2.4769303798675537\n",
      "2.476611852645874\n",
      "2.4763011932373047\n",
      "2.4759981632232666\n",
      "2.4757025241851807\n",
      "2.4754140377044678\n",
      "2.475132703781128\n",
      "2.474858045578003\n",
      "2.4745898246765137\n",
      "2.474327802658081\n",
      "2.474071979522705\n",
      "2.4738218784332275\n",
      "2.4735772609710693\n",
      "2.4733383655548096\n",
      "2.47310471534729\n",
      "2.47287654876709\n",
      "2.4726529121398926\n"
     ]
    }
   ],
   "source": [
    "#gradient descent\n",
    "\n",
    "for k in range(100):\n",
    "    #forward pass\n",
    "\txenc = F.one_hot(xs,num_classes = 27).float()#input to network:one hot encoding\n",
    "\tlogits = xenc@W #predict log-counts\n",
    "\tcounts = logits.exp()#counts equivalent to N\n",
    "\tprobs = counts/counts.sum(1,keepdims = True) #probabilities for next character\n",
    "\tloss = -probs[torch.arange(num),ys].log().mean()\n",
    "\tprint(loss.item())\n",
    "#Backward pass\n",
    "\tW.grad = None#\n",
    "\tloss.backward()\n",
    "\n",
    "#update \n",
    "\tW.data += -50*W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552f32b",
   "metadata": {},
   "source": [
    "<h2>Part II</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd840dd5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
